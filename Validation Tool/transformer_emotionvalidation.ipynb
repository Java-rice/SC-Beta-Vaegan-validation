{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69101bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import layers, Model\n",
    "\n",
    "# loading the EMOTHAW dataset\\n\",\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "def load_emothaw_data(directory_path):\n",
    "    all_data = []\n",
    "    all_labels = []\n",
    "\n",
    "    # Use glob to find all CSV files in the specified directory\n",
    "    file_paths = glob.glob(directory_path + \"/*.csv\")  # Modify for your file type, e.g., .npy or .txt\n",
    "\n",
    "    for file_path in file_paths:\n",
    "        # Extract the label from the filename (e.g., 'label_filename.csv')\n",
    "        label = os.path.basename(file_path).split('_')[0]  # Assuming the label is the part before the first '_'\n",
    "\n",
    "        # Load each file, assuming the first row contains the sequence length\n",
    "        df = pd.read_csv(file_path, header=None)\n",
    "\n",
    "        # Extract the sequence length from the first row\n",
    "        sequence_length = int(df.iloc[0, 0])\n",
    "\n",
    "        # Extract the feature data from the succeeding rows (excluding the first row)\n",
    "        data = df.iloc[1:, :].values  # All columns are features\n",
    "\n",
    "        # Reshape the data into (samples, sequence_length, features)\n",
    "        num_samples = data.shape[0] // sequence_length\n",
    "        num_features = data.shape[1]\n",
    "\n",
    "        # Reshape the data based on the sequence length\n",
    "        reshaped_data = data[:num_samples * sequence_length].reshape((num_samples, sequence_length, num_features))\n",
    "\n",
    "        # Append the reshaped data and labels to the lists\n",
    "        all_data.append(reshaped_data)\n",
    "\n",
    "        # The same label is assigned to all samples from this file\n",
    "        all_labels.append([label] * num_samples)\n",
    "\n",
    "    # Concatenate all data and labels from the files\n",
    "    all_data = np.concatenate(all_data, axis=0)\n",
    "    all_labels = np.concatenate(all_labels, axis=0)\n",
    "\n",
    "    return all_data, all_labels\n",
    "\n",
    "# Example usage: assuming your data files are stored in 'data/emothaw_files'\n",
    "data, labels = load_emothaw_data('data/emothaw_files')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "def load_emothaw_data(directory_path):\n",
    "    all_data = []\n",
    "    all_labels = []\n",
    "\n",
    "    # Use glob to find all CSV files in the specified directory\n",
    "    file_paths = glob.glob(directory_path + \"/*.csv\")  # Modify for your file type, e.g., .npy or .txt\n",
    "\n",
    "    for file_path in file_paths:\n",
    "        # Extract the label from the filename (e.g., 'label_filename.csv')\n",
    "        label = os.path.basename(file_path).split('_')[0]  # Assuming the label is the part before the first '_'\n",
    "\n",
    "        # Load each file, assuming the first row contains the sequence length\n",
    "        df = pd.read_csv(file_path, header=None)\n",
    "\n",
    "        # Extract the sequence length from the first row\n",
    "        sequence_length = int(df.iloc[0, 0])\n",
    "\n",
    "        # Extract the feature data from the succeeding rows (excluding the first row)\n",
    "        data = df.iloc[1:, :].values  # All columns are features\n",
    "\n",
    "        # Reshape the data into (samples, sequence_length, features)\n",
    "        num_samples = data.shape[0] // sequence_length\n",
    "        num_features = data.shape[1]\n",
    "\n",
    "        # Reshape the data based on the sequence length\n",
    "        reshaped_data = data[:num_samples * sequence_length].reshape((num_samples, sequence_length, num_features))\n",
    "\n",
    "        # Append the reshaped data and labels to the lists\n",
    "        all_data.append(reshaped_data)\n",
    "\n",
    "        # The same label is assigned to all samples from this file\n",
    "        all_labels.append([label] * num_samples)\n",
    "\n",
    "    # Concatenate all data and labels from the files\n",
    "    all_data = np.concatenate(all_data, axis=0)\n",
    "    all_labels = np.concatenate(all_labels, axis=0)\n",
    "\n",
    "    return all_data, all_labels\n",
    "\n",
    "# Example usage: assuming your data files are stored in 'data/emothaw_files'\n",
    "data, labels = load_emothaw_data('data/emothaw_files')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f430eaf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Extraction\\n\",\n",
    "# EMOTHAW dataset, a 3D array (samples, sequence length, features)\n",
    "\n",
    "# Extracting time domain features\\n\n",
    "def extract_time_domain_features(data):\n",
    "    time_features = []\n",
    "    for sample in data:\n",
    "        sample_features = []\n",
    "        for feature in sample.T:  # Assuming the features are along the last axis\\n\",\n",
    "            feature_stats = [\n",
    "                np.mean(feature),\n",
    "                np.std(feature),\n",
    "                np.min(feature),\n",
    "                np.max(feature),\n",
    "                np.percentile(feature, 25),\n",
    "                np.percentile(feature, 50),\n",
    "                np.percentile(feature, 75),\n",
    "            ]\n",
    "        time_features.append(sample_features)\n",
    "    return np.array(time_features)\n",
    "\n",
    "time_domain_features = extract_time_domain_features(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830915a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Extracting frequency domain features using FFT\n",
    "def extract_frequency_domain_features(data):\n",
    "    freq_features = []\n",
    "    for sample in data:\n",
    "        sample_features = []\n",
    "        for feature in sample.T:  # Assuming the features are along the last axis\n",
    "            freq_feature = np.fft.fft(feature)\n",
    "            freq_magnitude = np.abs(freq_feature)\n",
    "            dominant_freq = np.argmax(freq_magnitude)\n",
    "            freq_energy = np.sum(freq_magnitude)\n",
    "            sample_features.extend([dominant_freq, freq_energy])\n",
    "        freq_features.append(sample_features)\n",
    "    return np.array(freq_features)\n",
    "\n",
    "frequency_domain_features = extract_frequency_domain_features(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d920d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting statistical features\n",
    "def extract_statistical_features(data):\n",
    "    statistical_features = []\n",
    "    for sample in data:\n",
    "        sample_features = []\n",
    "        for feature in sample.T:  # The features are along the last axis\n",
    "            feature_stats = [\n",
    "                np.mean(feature),\n",
    "                np.median(feature),\n",
    "                np.var(feature),\n",
    "                np.skew(feature),\n",
    "                np.kurtosis(feature)\n",
    "            ]\n",
    "            sample_features.extend(feature_stats)\n",
    "        statistical_features.append(sample_features)\n",
    "    return np.array(statistical_features)\n",
    "\n",
    "statistical_features = extract_statistical_features(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9455a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying Attention-based Transformer Model\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "\n",
    "# Applying Attention-based Transformer Model\n",
    "class MultiHeadSelfAttention(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        if embed_dim % num_heads != 0:\n",
    "            raise ValueError(f\"embedding dimension = {embed_dim} should be divisible by number of heads = {num_heads}\")\n",
    "        self.projection_dim = embed_dim // num_heads\n",
    "        self.query_dense = layers.Dense(embed_dim)\n",
    "        self.key_dense = layers.Dense(embed_dim)\n",
    "        self.value_dense = layers.Dense(embed_dim)\n",
    "        self.combine_heads = layers.Dense(embed_dim)\n",
    "\n",
    "    def attention(self, query, key, value):\n",
    "        score = tf.matmul(query, key, transpose_b=True)\n",
    "        dim_key = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "        scaled_score = score / tf.math.sqrt(dim_key)\n",
    "        weights = tf.nn.softmax(scaled_score, axis=-1)\n",
    "        output = tf.matmul(weights, value)\n",
    "        return output, weights\n",
    "\n",
    "    def separate_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        query = self.query_dense(inputs)\n",
    "        key = self.key_dense(inputs)\n",
    "        value = self.value_dense(inputs)\n",
    "        query = self.separate_heads(query, batch_size)\n",
    "        key = self.separate_heads(key, batch_size)\n",
    "        value = self.separate_heads(value, batch_size)\n",
    "        attention, weights = self.attention(query, key, value)\n",
    "        attention = tf.transpose(attention, perm=[0, 2, 1, 3])\n",
    "        concat_attention = tf.reshape(attention, (batch_size, -1, self.embed_dim))\n",
    "        output = self.combine_heads(concat_attention)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63464e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = MultiHeadSelfAttention(embed_dim, num_heads)\n",
    "        self.ffn = tf.keras.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim)]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef508e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class TransformerClassifier(Model):\n",
    "    def __init__(self, num_classes, embed_dim, num_heads, ff_dim, num_layers):\n",
    "        super(TransformerClassifier, self).__init__()\n",
    "        self.transformer_blocks = [\n",
    "            TransformerBlock(embed_dim, num_heads, ff_dim) for _ in range(num_layers)\n",
    "        ]\n",
    "        self.global_average_pooling = layers.GlobalAveragePooling1D()\n",
    "        self.dense = layers.Dense(num_classes, activation='softmax')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = inputs\n",
    "        for transformer in self.transformer_blocks:\n",
    "            x = transformer(x)\n",
    "        x = self.global_average_pooling(x)\n",
    "        return self.dense(x)\n",
    "\n",
    "# Splitting the data into training and testing sets\n",
    "features = np.concatenate((time_domain_features, frequency_domain_features, statistical_features), axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Defining the model parameters\n",
    "num_classes = 3\n",
    "embed_dim = 128\n",
    "num_heads = 4\n",
    "ff_dim = 128\n",
    "num_layers = 2\n",
    "\n",
    "# Creating the Transformer model\n",
    "model = TransformerClassifier(num_classes, embed_dim, num_heads, ff_dim, num_layers)\n",
    "\n",
    "# Compiling the model\n",
    "model.compile(optimizer=Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Training the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Evaluating the model\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8ad576",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset splitting in Training/testing\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# EMOTHAW dataset\n",
    "def generate_sample_data(num_samples, seq_len, num_features, num_classes):\n",
    "    data = np.random.rand(num_samples, seq_len, num_features)\n",
    "    labels = np.random.randint(0, num_classes, num_samples)\n",
    "    return data, labels\n",
    "\n",
    "# Sample of EMOTHAW dataset\\n\"\n",
    "num_samples = 1000\n",
    "seq_length = 50\n",
    "num_features = 128\n",
    "num_classes = 3  # Depression, Anxiety and Stress\\n\",\n",
    "\n",
    "data, labels = generate_sample_data(num_samples, seq_length, num_features, num_classes)\n",
    "\n",
    "# Split the data into training and testing sets\\n\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "\"\\n\",\n",
    "# Build and compile the model\\n\n",
    "num_blocks = 3\n",
    "embed_dim = 64\n",
    "num_heads = 4\n",
    "ff_dim = 128\n",
    "learning_rate = 0.001\n",
    "\n",
    "transformer_model = TransformerClassifier(num_classes, embed_dim, num_heads, ff_dim, num_blocks)\n",
    "transformer_model.compile(optimizer=Adam(learning_rate), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Training the model\n",
    "epochs = 25\n",
    "batch_size = 32\n",
    "\n",
    "transformer_model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluating the model\n",
    "loss, accuracy = transformer_model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46a3252",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from keras import Sequential\n",
    "from tensorflow import keras\n",
    "import os\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Flatten(input_shape=(28, 28)))\n",
    "model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=3)\n",
    "#model.save('Emotion-detection.model')\\n\",\n",
    "\n",
    "# Compile the model\\n\",\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
