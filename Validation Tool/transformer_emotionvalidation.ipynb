{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69101bf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: ../test/samplefew\\0_hw00001(2).svc\n",
      "Shape of data from file ../test/samplefew\\0_hw00001(2).svc: (3444, 7)\n",
      "Processing file: ../test/samplefew\\0_hw00001(2)21.svc\n",
      "Shape of data from file ../test/samplefew\\0_hw00001(2)21.svc: (2086, 7)\n",
      "Processing file: ../test/samplefew\\0_hw00003(2).svc\n",
      "Shape of data from file ../test/samplefew\\0_hw00003(2).svc: (3959, 7)\n",
      "Processing file: ../test/samplefew\\0_hw00003.svc\n",
      "Shape of data from file ../test/samplefew\\0_hw00003.svc: (3102, 7)\n",
      "Processing file: ../test/samplefew\\0_hw00005(2).svc\n",
      "Shape of data from file ../test/samplefew\\0_hw00005(2).svc: (1531, 7)\n",
      "Processing file: ../test/samplefew\\0_hw00006(2).svc\n",
      "Shape of data from file ../test/samplefew\\0_hw00006(2).svc: (4588, 7)\n",
      "Processing file: ../test/samplefew\\0_hw00006(3).svc\n",
      "Shape of data from file ../test/samplefew\\0_hw00006(3).svc: (4094, 7)\n",
      "Processing file: ../test/samplefew\\0_hw00007(2).svc\n",
      "Shape of data from file ../test/samplefew\\0_hw00007(2).svc: (3457, 7)\n",
      "Processing file: ../test/samplefew\\1_hw00001.svc\n",
      "Shape of data from file ../test/samplefew\\1_hw00001.svc: (1474, 7)\n",
      "Processing file: ../test/samplefew\\1_hw00002(2).svc\n",
      "Shape of data from file ../test/samplefew\\1_hw00002(2).svc: (3221, 7)\n",
      "Processing file: ../test/samplefew\\1_hw00002.svc\n",
      "Shape of data from file ../test/samplefew\\1_hw00002.svc: (2735, 7)\n",
      "Processing file: ../test/samplefew\\1_hw00003(2).svc\n",
      "Shape of data from file ../test/samplefew\\1_hw00003(2).svc: (4401, 7)\n",
      "Processing file: ../test/samplefew\\1_hw00003.svc\n",
      "Shape of data from file ../test/samplefew\\1_hw00003.svc: (4276, 7)\n",
      "Processing file: ../test/samplefew\\1_hw00004(2).svc\n",
      "Shape of data from file ../test/samplefew\\1_hw00004(2).svc: (2315, 7)\n",
      "Processing file: ../test/samplefew\\1_hw00004(3).svc\n",
      "Shape of data from file ../test/samplefew\\1_hw00004(3).svc: (3527, 7)\n",
      "Processing file: ../test/samplefew\\1_hw00005 (2).svc\n",
      "Shape of data from file ../test/samplefew\\1_hw00005 (2).svc: (862, 7)\n",
      "Processing file: ../test/samplefew\\1_hw00005.svc\n",
      "Shape of data from file ../test/samplefew\\1_hw00005.svc: (1208, 7)\n",
      "Processing file: ../test/samplefew\\1_hw00006.svc\n",
      "Shape of data from file ../test/samplefew\\1_hw00006.svc: (2507, 7)\n",
      "Processing file: ../test/samplefew\\1_hw00007.svc\n",
      "Shape of data from file ../test/samplefew\\1_hw00007.svc: (2985, 7)\n",
      "Processing file: ../test/samplefew\\2_hw00001.svc\n",
      "Shape of data from file ../test/samplefew\\2_hw00001.svc: (2085, 7)\n",
      "Processing file: ../test/samplefew\\2_hw00002(2).svc\n",
      "Shape of data from file ../test/samplefew\\2_hw00002(2).svc: (5553, 7)\n",
      "Processing file: ../test/samplefew\\2_hw00002.svc\n",
      "Shape of data from file ../test/samplefew\\2_hw00002.svc: (3800, 7)\n",
      "Processing file: ../test/samplefew\\2_hw00004 (2).svc\n",
      "Shape of data from file ../test/samplefew\\2_hw00004 (2).svc: (1111, 7)\n",
      "Processing file: ../test/samplefew\\2_hw00004.svc\n",
      "Shape of data from file ../test/samplefew\\2_hw00004.svc: (1951, 7)\n",
      "Processing file: ../test/samplefew\\2_hw00005(2).svc\n",
      "Shape of data from file ../test/samplefew\\2_hw00005(2).svc: (1852, 7)\n",
      "Processing file: ../test/samplefew\\2_hw00006.svc\n",
      "Shape of data from file ../test/samplefew\\2_hw00006.svc: (3178, 7)\n",
      "Processing file: ../test/samplefew\\2_hw00007(2).svc\n",
      "Shape of data from file ../test/samplefew\\2_hw00007(2).svc: (3482, 7)\n",
      "Shapes of all_data before concatenation:\n",
      "Data array 0 shape: (3444, 7)\n",
      "Data array 1 shape: (2086, 7)\n",
      "Data array 2 shape: (3959, 7)\n",
      "Data array 3 shape: (3102, 7)\n",
      "Data array 4 shape: (1531, 7)\n",
      "Data array 5 shape: (4588, 7)\n",
      "Data array 6 shape: (4094, 7)\n",
      "Data array 7 shape: (3457, 7)\n",
      "Data array 8 shape: (1474, 7)\n",
      "Data array 9 shape: (3221, 7)\n",
      "Data array 10 shape: (2735, 7)\n",
      "Data array 11 shape: (4401, 7)\n",
      "Data array 12 shape: (4276, 7)\n",
      "Data array 13 shape: (2315, 7)\n",
      "Data array 14 shape: (3527, 7)\n",
      "Data array 15 shape: (862, 7)\n",
      "Data array 16 shape: (1208, 7)\n",
      "Data array 17 shape: (2507, 7)\n",
      "Data array 18 shape: (2985, 7)\n",
      "Data array 19 shape: (2085, 7)\n",
      "Data array 20 shape: (5553, 7)\n",
      "Data array 21 shape: (3800, 7)\n",
      "Data array 22 shape: (1111, 7)\n",
      "Data array 23 shape: (1951, 7)\n",
      "Data array 24 shape: (1852, 7)\n",
      "Data array 25 shape: (3178, 7)\n",
      "Data array 26 shape: (3482, 7)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "def load_emothaw_data(directory_path):\n",
    "    all_data = []\n",
    "    all_labels = []\n",
    "\n",
    "    # Use glob to find all SVC files in the specified directory\n",
    "    file_paths = glob.glob(os.path.join(directory_path, \"*.svc\"))\n",
    "    if not file_paths:\n",
    "        raise ValueError(\"No files found in the specified directory\")\n",
    "\n",
    "    for file_path in file_paths:\n",
    "        print(f\"Processing file: {file_path}\")\n",
    "        try:\n",
    "            # Extract the label from the filename\n",
    "            label = int(os.path.basename(file_path).split('_')[0])  # Assuming the label is the part before the first '_'\n",
    "\n",
    "            # Load the file and remove trailing spaces\n",
    "            with open(file_path, 'r') as f:\n",
    "                lines = [line.strip() for line in f.readlines() if line.strip()]  # Strip whitespace and ignore empty lines\n",
    "\n",
    "            # Process lines into a DataFrame by splitting by whitespace\n",
    "            df = pd.DataFrame([line.split() for line in lines])  # Split by whitespace\n",
    "\n",
    "            if df.empty:\n",
    "                print(f\"Warning: {file_path} is empty after processing. Skipping file.\")\n",
    "                continue\n",
    "\n",
    "            # The first row gives the number of rows starting from the second row\n",
    "            total_rows = int(df.iloc[0, 0])  # Number of sequences\n",
    "\n",
    "            # Extract the feature data from the rows starting from the second row\n",
    "            data = df.iloc[1:, :].values  # All columns are features\n",
    "\n",
    "            if data.shape[0] < total_rows:\n",
    "                raise ValueError(f\"Insufficient data: expected {total_rows}, but found {data.shape[0]}\")\n",
    "\n",
    "            # Reshape the data to ensure each row has 7 values\n",
    "            reshaped_data = []\n",
    "            for row in data:\n",
    "                # Ensure each row has exactly 7 columns\n",
    "                if len(row) == 7:\n",
    "                    reshaped_data.append(row)\n",
    "                else:\n",
    "                    print(f\"Warning: Row does not have 7 values, skipping: {row}\")\n",
    "\n",
    "            # Convert the list of rows to a NumPy array only if not empty\n",
    "            if reshaped_data:\n",
    "                reshaped_data = np.array(reshaped_data, dtype=float)  # Ensure float type\n",
    "\n",
    "                # Check that reshaped_data has enough rows\n",
    "                if reshaped_data.shape[0] < total_rows:\n",
    "                    raise ValueError(f\"Insufficient data: expected {total_rows}, but found {reshaped_data.shape[0]}\")\n",
    "\n",
    "                # Append the reshaped data and labels to the lists\n",
    "                all_data.append(reshaped_data)\n",
    "                all_labels.append([label] * reshaped_data.shape[0])\n",
    "\n",
    "                # Print the shape of reshaped data for debugging\n",
    "                print(f\"Shape of data from file {file_path}: {reshaped_data.shape}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {file_path}: {e}\")\n",
    "            continue\n",
    "\n",
    "    if not all_data or not all_labels:\n",
    "        raise ValueError(\"No valid data found in the directory\")\n",
    "\n",
    "    # Display shapes of individual entries in all_data\n",
    "    print(\"Shapes of all_data before concatenation:\")\n",
    "    for i, data_array in enumerate(all_data):\n",
    "        print(f\"Data array {i} shape: {data_array.shape}\")\n",
    "\n",
    "    # Attempt to concatenate all data and labels from the files\n",
    "    try:\n",
    "        all_data = np.concatenate(all_data, axis=0)  # Concatenate along the sample axis\n",
    "    except Exception as e:\n",
    "        print(f\"Error during concatenation: {e}\")\n",
    "\n",
    "    all_labels = np.concatenate(all_labels, axis=0)  # Concatenate labels\n",
    "\n",
    "    return all_data, all_labels\n",
    "\n",
    "# Example usage\n",
    "data, labels = load_emothaw_data(r'../test/samplefew/')  # Use raw string for paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f430eaf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-13 14:36:00,457 - Starting time-domain feature extraction\n",
      "Extracting features: 100%|██████████| 78784/78784 [05:42<00:00, 229.89sample/s]\n",
      "2024-10-13 14:41:43,368 - Feature extraction completed in 0:05:42.910339\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# Function to extract time-domain features with progress bar and logging\n",
    "def extract_time_domain_features(data):\n",
    "    logger.info(\"Starting time-domain feature extraction\")\n",
    "    start_time = datetime.now()\n",
    "\n",
    "    time_features = []\n",
    "    \n",
    "    # Use tqdm to create a progress bar\n",
    "    for sample in tqdm(data, desc=\"Extracting features\", unit=\"sample\"):\n",
    "        sample_features = []\n",
    "        for feature in sample.T:  # Assuming the features are along the last axis\n",
    "            feature_stats = [\n",
    "                np.mean(feature),\n",
    "                np.std(feature),\n",
    "                np.min(feature),\n",
    "                np.max(feature),\n",
    "                np.percentile(feature, 25),\n",
    "                np.percentile(feature, 50),\n",
    "                np.percentile(feature, 75),\n",
    "            ]\n",
    "            sample_features.extend(feature_stats)  # Append the computed stats to sample_features\n",
    "        time_features.append(sample_features)  # Append the features for the sample\n",
    "\n",
    "    end_time = datetime.now()\n",
    "    elapsed_time = end_time - start_time\n",
    "    logger.info(f\"Feature extraction completed in {elapsed_time}\")\n",
    "\n",
    "    return np.array(time_features)\n",
    "\n",
    "# Assuming 'data' is your input 3D array (samples, sequence length, features)\n",
    "time_domain_features = extract_time_domain_features(data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad8fc6f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.0962e+04 0.0000e+00 5.0962e+04 ... 1.5000e+01 1.5000e+01 1.5000e+01]\n",
      " [5.0962e+04 0.0000e+00 5.0962e+04 ... 4.5000e+01 4.5000e+01 4.5000e+01]\n",
      " [5.0962e+04 0.0000e+00 5.0962e+04 ... 7.5000e+01 7.5000e+01 7.5000e+01]\n",
      " ...\n",
      " [1.2224e+04 0.0000e+00 1.2224e+04 ... 8.0600e+02 8.0600e+02 8.0600e+02]\n",
      " [1.2236e+04 0.0000e+00 1.2236e+04 ... 5.6800e+02 5.6800e+02 5.6800e+02]\n",
      " [1.2252e+04 0.0000e+00 1.2252e+04 ... 8.1000e+01 8.1000e+01 8.1000e+01]]\n"
     ]
    }
   ],
   "source": [
    "print(time_domain_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "830915a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-13 14:41:43,827 - Starting frequency-domain feature extraction\n",
      "Extracting frequency features: 100%|██████████| 78784/78784 [00:02<00:00, 35451.51sample/s]\n",
      "2024-10-13 14:41:46,055 - Frequency-domain feature extraction completed in 0:00:02.226384\n",
      "2024-10-13 14:41:46,119 - Data: [[5.0962000e+04 3.4188000e+04 1.6718871e+07 ... 1.9500000e+03\n",
      "  6.2000000e+02 1.5000000e+01]\n",
      " [5.0962000e+04 3.4188000e+04 1.6718878e+07 ... 1.9500000e+03\n",
      "  6.2000000e+02 4.5000000e+01]\n",
      " [5.0962000e+04 3.4188000e+04 1.6718886e+07 ... 1.9500000e+03\n",
      "  6.2000000e+02 7.5000000e+01]\n",
      " ...\n",
      " [1.2224000e+04 2.9560000e+04 1.7049425e+07 ... 1.1900000e+03\n",
      "  8.0000000e+02 8.0600000e+02]\n",
      " [1.2236000e+04 2.9560000e+04 1.7049433e+07 ... 1.2200000e+03\n",
      "  8.1000000e+02 5.6800000e+02]\n",
      " [1.2252000e+04 2.9560000e+04 1.7049440e+07 ... 1.2200000e+03\n",
      "  8.1000000e+02 8.1000000e+01]]\n",
      "2024-10-13 14:41:46,121 - Frequency Domain Features: [[0.00000000e+00 1.17032471e+08]\n",
      " [0.00000000e+00 1.17032520e+08]\n",
      " [0.00000000e+00 1.17032576e+08]\n",
      " ...\n",
      " [0.00000000e+00 1.19346077e+08]\n",
      " [0.00000000e+00 1.19346133e+08]\n",
      " [0.00000000e+00 1.19346182e+08]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# Extracting frequency domain features using FFT with progress bar and logging\n",
    "def extract_frequency_domain_features(data):\n",
    "    logger.info(\"Starting frequency-domain feature extraction\")\n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    freq_features = []\n",
    "    \n",
    "    # Ensure the data has the correct dimensions\n",
    "    if len(data.shape) != 2:\n",
    "        raise ValueError(f\"Expected data with 2 dimensions (samples, features), but got {data.shape}\")\n",
    "    \n",
    "    # Use tqdm to create a progress bar\n",
    "    for sample in tqdm(data, desc=\"Extracting frequency features\", unit=\"sample\"):\n",
    "        sample_features = []\n",
    "        \n",
    "        # Check if the sample is a 1D array\n",
    "        if len(sample.shape) != 1:\n",
    "            raise ValueError(f\"Expected each sample to be 1D, but got {sample.shape}\")\n",
    "\n",
    "        # Apply FFT to the sample (which is already 1D, hence no need for .T)\n",
    "        freq_feature = np.fft.fft(sample)\n",
    "        freq_magnitude = np.abs(freq_feature)\n",
    "        dominant_freq = np.argmax(freq_magnitude)\n",
    "        freq_energy = np.sum(freq_magnitude)\n",
    "        \n",
    "        sample_features.extend([dominant_freq, freq_energy])\n",
    "        freq_features.append(sample_features)\n",
    "    \n",
    "    end_time = datetime.now()\n",
    "    elapsed_time = end_time - start_time\n",
    "    logger.info(f\"Frequency-domain feature extraction completed in {elapsed_time}\")\n",
    "\n",
    "    return np.array(freq_features)\n",
    "\n",
    "# Example usage: Assuming `data` is a 2D array (samples, features)\n",
    "# data = np.random.rand(100, 50)  # Example data\n",
    "frequency_domain_features = extract_frequency_domain_features(data)\n",
    "\n",
    "# Log the output\n",
    "logger.info(f\"Data: {data}\")\n",
    "logger.info(f\"Frequency Domain Features: {frequency_domain_features}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19706a1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00000000e+00 1.17032471e+08]\n",
      " [0.00000000e+00 1.17032520e+08]\n",
      " [0.00000000e+00 1.17032576e+08]\n",
      " ...\n",
      " [0.00000000e+00 1.19346077e+08]\n",
      " [0.00000000e+00 1.19346133e+08]\n",
      " [0.00000000e+00 1.19346182e+08]]\n"
     ]
    }
   ],
   "source": [
    "print(frequency_domain_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6a7bf958",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-13 15:43:19,543 - Starting statistical feature extraction\n",
      "Extracting statistical features: 100%|██████████| 78784/78784 [04:01<00:00, 326.83sample/s]\n",
      "2024-10-13 15:47:20,600 - Statistical feature extraction completed in 0:04:01.056327\n",
      "2024-10-13 15:47:20,888 - Data: [[5.0962000e+04 3.4188000e+04 1.6718871e+07 ... 1.9500000e+03\n",
      "  6.2000000e+02 1.5000000e+01]\n",
      " [5.0962000e+04 3.4188000e+04 1.6718878e+07 ... 1.9500000e+03\n",
      "  6.2000000e+02 4.5000000e+01]\n",
      " [5.0962000e+04 3.4188000e+04 1.6718886e+07 ... 1.9500000e+03\n",
      "  6.2000000e+02 7.5000000e+01]\n",
      " ...\n",
      " [1.2224000e+04 2.9560000e+04 1.7049425e+07 ... 1.1900000e+03\n",
      "  8.0000000e+02 8.0600000e+02]\n",
      " [1.2236000e+04 2.9560000e+04 1.7049433e+07 ... 1.2200000e+03\n",
      "  8.1000000e+02 5.6800000e+02]\n",
      " [1.2252000e+04 2.9560000e+04 1.7049440e+07 ... 1.2200000e+03\n",
      "  8.1000000e+02 8.1000000e+01]]\n",
      "2024-10-13 15:47:20,891 - Statistical Features: [[5.09620000e+04 5.09620000e+04 0.00000000e+00 ... 0.00000000e+00\n",
      "  2.04119691e+00 2.16656956e+00]\n",
      " [5.09620000e+04 5.09620000e+04 0.00000000e+00 ... 0.00000000e+00\n",
      "  2.04119692e+00 2.16656959e+00]\n",
      " [5.09620000e+04 5.09620000e+04 0.00000000e+00 ... 0.00000000e+00\n",
      "  2.04119694e+00 2.16656962e+00]\n",
      " ...\n",
      " [1.22240000e+04 1.22240000e+04 0.00000000e+00 ... 0.00000000e+00\n",
      "  2.04122950e+00 2.16664061e+00]\n",
      " [1.22360000e+04 1.22360000e+04 0.00000000e+00 ... 0.00000000e+00\n",
      "  2.04122945e+00 2.16664051e+00]\n",
      " [1.22520000e+04 1.22520000e+04 0.00000000e+00 ... 0.00000000e+00\n",
      "  2.04122933e+00 2.16664024e+00]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import skew, kurtosis\n",
    "from datetime import datetime\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# Function to extract statistical features with progress bar and logging\n",
    "def extract_statistical_features(data):\n",
    "    logger.info(\"Starting statistical feature extraction\")\n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    statistical_features = []\n",
    "    \n",
    "    for sample in tqdm(data, desc=\"Extracting statistical features\", unit=\"sample\"):\n",
    "        sample_features = []\n",
    "        skew_value = skew(sample.T)\n",
    "        kurtosis_value = kurtosis(sample.T)\n",
    "        for feature in sample.T:\n",
    "            mean = np.mean(feature)\n",
    "            median = np.median(feature)\n",
    "            variance = np.var(feature)\n",
    "            feature_stats = [mean, median, variance, skew_value, kurtosis_value]\n",
    "            sample_features.extend(feature_stats)\n",
    "        statistical_features.append(sample_features)\n",
    "    \n",
    "    end_time = datetime.now()\n",
    "    elapsed_time = end_time - start_time\n",
    "    logger.info(f\"Statistical feature extraction completed in {elapsed_time}\")\n",
    "    \n",
    "    return np.array(statistical_features)\n",
    "\n",
    "# Example usage: Assuming 'data' is a 3D array (samples, sequence length, features)\n",
    "statistical_features = extract_statistical_features(data)\n",
    "\n",
    "# Log the output\n",
    "logger.info(f\"Data: {data}\")\n",
    "logger.info(f\"Statistical Features: {statistical_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "929a5094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.0962e+04 0.0000e+00 5.0962e+04 ... 1.5000e+01 1.5000e+01 1.5000e+01]\n",
      " [5.0962e+04 0.0000e+00 5.0962e+04 ... 4.5000e+01 4.5000e+01 4.5000e+01]\n",
      " [5.0962e+04 0.0000e+00 5.0962e+04 ... 7.5000e+01 7.5000e+01 7.5000e+01]\n",
      " ...\n",
      " [1.2224e+04 0.0000e+00 1.2224e+04 ... 8.0600e+02 8.0600e+02 8.0600e+02]\n",
      " [1.2236e+04 0.0000e+00 1.2236e+04 ... 5.6800e+02 5.6800e+02 5.6800e+02]\n",
      " [1.2252e+04 0.0000e+00 1.2252e+04 ... 8.1000e+01 8.1000e+01 8.1000e+01]]\n",
      "[[0.00000000e+00 1.17032471e+08]\n",
      " [0.00000000e+00 1.17032520e+08]\n",
      " [0.00000000e+00 1.17032576e+08]\n",
      " ...\n",
      " [0.00000000e+00 1.19346077e+08]\n",
      " [0.00000000e+00 1.19346133e+08]\n",
      " [0.00000000e+00 1.19346182e+08]]\n",
      "[[5.09620000e+04 5.09620000e+04 0.00000000e+00 ... 0.00000000e+00\n",
      "  2.04119691e+00 2.16656956e+00]\n",
      " [5.09620000e+04 5.09620000e+04 0.00000000e+00 ... 0.00000000e+00\n",
      "  2.04119692e+00 2.16656959e+00]\n",
      " [5.09620000e+04 5.09620000e+04 0.00000000e+00 ... 0.00000000e+00\n",
      "  2.04119694e+00 2.16656962e+00]\n",
      " ...\n",
      " [1.22240000e+04 1.22240000e+04 0.00000000e+00 ... 0.00000000e+00\n",
      "  2.04122950e+00 2.16664061e+00]\n",
      " [1.22360000e+04 1.22360000e+04 0.00000000e+00 ... 0.00000000e+00\n",
      "  2.04122945e+00 2.16664051e+00]\n",
      " [1.22520000e+04 1.22520000e+04 0.00000000e+00 ... 0.00000000e+00\n",
      "  2.04122933e+00 2.16664024e+00]]\n"
     ]
    }
   ],
   "source": [
    "print(time_domain_features)\n",
    "print(frequency_domain_features)\n",
    "print(statistical_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e9455a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying Attention-based Transformer Model\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "\n",
    "# Applying Attention-based Transformer Model\n",
    "class MultiHeadSelfAttention(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        if embed_dim % num_heads != 0:\n",
    "            raise ValueError(f\"embedding dimension = {embed_dim} should be divisible by number of heads = {num_heads}\")\n",
    "        self.projection_dim = embed_dim // num_heads\n",
    "        self.query_dense = layers.Dense(embed_dim)\n",
    "        self.key_dense = layers.Dense(embed_dim)\n",
    "        self.value_dense = layers.Dense(embed_dim)\n",
    "        self.combine_heads = layers.Dense(embed_dim)\n",
    "\n",
    "    def attention(self, query, key, value):\n",
    "        score = tf.matmul(query, key, transpose_b=True)\n",
    "        dim_key = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "        scaled_score = score / tf.math.sqrt(dim_key)\n",
    "        weights = tf.nn.softmax(scaled_score, axis=-1)\n",
    "        output = tf.matmul(weights, value)\n",
    "        return output, weights\n",
    "\n",
    "    def separate_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        query = self.query_dense(inputs)\n",
    "        key = self.key_dense(inputs)\n",
    "        value = self.value_dense(inputs)\n",
    "        query = self.separate_heads(query, batch_size)\n",
    "        key = self.separate_heads(key, batch_size)\n",
    "        value = self.separate_heads(value, batch_size)\n",
    "        attention, weights = self.attention(query, key, value)\n",
    "        attention = tf.transpose(attention, perm=[0, 2, 1, 3])\n",
    "        concat_attention = tf.reshape(attention, (batch_size, -1, self.embed_dim))\n",
    "        output = self.combine_heads(concat_attention)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "63464e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = MultiHeadSelfAttention(embed_dim, num_heads)\n",
    "        self.ffn = tf.keras.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim)]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5ef508e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1576/1576\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 47ms/step - accuracy: 0.4757 - loss: 1.0303 - val_accuracy: 0.5965 - val_loss: 0.9220\n",
      "Epoch 2/10\n",
      "\u001b[1m1576/1576\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 47ms/step - accuracy: 0.5989 - loss: 0.8011 - val_accuracy: 0.6081 - val_loss: 0.7540\n",
      "Epoch 3/10\n",
      "\u001b[1m1576/1576\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 42ms/step - accuracy: 0.6815 - loss: 0.6424 - val_accuracy: 0.7194 - val_loss: 0.4767\n",
      "Epoch 4/10\n",
      "\u001b[1m1576/1576\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 40ms/step - accuracy: 0.7405 - loss: 0.5223 - val_accuracy: 0.8141 - val_loss: 0.3650\n",
      "Epoch 5/10\n",
      "\u001b[1m1576/1576\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 39ms/step - accuracy: 0.7702 - loss: 0.4533 - val_accuracy: 0.6593 - val_loss: 0.6718\n",
      "Epoch 6/10\n",
      "\u001b[1m1576/1576\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 36ms/step - accuracy: 0.7748 - loss: 0.4593 - val_accuracy: 0.8350 - val_loss: 0.3351\n",
      "Epoch 7/10\n",
      "\u001b[1m1576/1576\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 36ms/step - accuracy: 0.7910 - loss: 0.4007 - val_accuracy: 0.8983 - val_loss: 0.3058\n",
      "Epoch 8/10\n",
      "\u001b[1m1576/1576\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 36ms/step - accuracy: 0.8096 - loss: 0.3895 - val_accuracy: 0.8541 - val_loss: 0.3235\n",
      "Epoch 9/10\n",
      "\u001b[1m1576/1576\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 37ms/step - accuracy: 0.8186 - loss: 0.3803 - val_accuracy: 0.6971 - val_loss: 0.6144\n",
      "Epoch 10/10\n",
      "\u001b[1m1576/1576\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 37ms/step - accuracy: 0.8221 - loss: 0.3704 - val_accuracy: 0.8739 - val_loss: 0.3216\n",
      "\u001b[1m493/493\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 12ms/step - accuracy: 0.8768 - loss: 0.3179\n",
      "Test Loss: 0.31927290558815, Test Accuracy: 0.8758012056350708\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import layers, Model\n",
    "\n",
    "class TransformerClassifier(Model):\n",
    "    def __init__(self, num_classes, embed_dim, num_heads, ff_dim, num_layers):\n",
    "        super(TransformerClassifier, self).__init__()\n",
    "        self.dense_input = layers.Dense(embed_dim)  # Add this layer to project input features\n",
    "        self.transformer_blocks = [\n",
    "            TransformerBlock(embed_dim, num_heads, ff_dim) for _ in range(num_layers)\n",
    "        ]\n",
    "        self.global_average_pooling = layers.GlobalAveragePooling1D()\n",
    "        self.dense = layers.Dense(num_classes, activation='softmax')\n",
    "\n",
    "    def call(self, inputs, training=False):  # Add training parameter\n",
    "        x = self.dense_input(inputs)  # Project inputs to the embedding dimension\n",
    "        for transformer in self.transformer_blocks:\n",
    "            x = transformer(x, training=training)  # Pass training to the transformer\n",
    "        x = self.global_average_pooling(x)\n",
    "        return self.dense(x)\n",
    "\n",
    "# Splitting the data into training and testing sets\n",
    "features = np.concatenate((time_domain_features, frequency_domain_features, statistical_features), axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Defining the model parameters\n",
    "num_classes = 3\n",
    "embed_dim = 128\n",
    "num_heads = 4\n",
    "ff_dim = 128\n",
    "num_layers = 2\n",
    "\n",
    "# Creating the Transformer model\n",
    "model = TransformerClassifier(num_classes, embed_dim, num_heads, ff_dim, num_layers)\n",
    "\n",
    "# Compiling the model\n",
    "model.compile(optimizer=Adam(clipnorm=1.0), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Training the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Evaluating the model\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fe8ad576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 14s/step - accuracy: 0.0000e+00 - loss: 2.5161 - val_accuracy: 1.0000 - val_loss: 0.5597\n",
      "Epoch 2/25\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 595ms/step - accuracy: 0.6000 - loss: 0.7050 - val_accuracy: 1.0000 - val_loss: 0.3448\n",
      "Epoch 3/25\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 604ms/step - accuracy: 0.6000 - loss: 0.7004 - val_accuracy: 1.0000 - val_loss: 0.5051\n",
      "Epoch 4/25\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 633ms/step - accuracy: 0.6000 - loss: 0.6652 - val_accuracy: 0.0000e+00 - val_loss: 0.7056\n",
      "Epoch 5/25\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 590ms/step - accuracy: 0.6000 - loss: 0.6873 - val_accuracy: 1.0000 - val_loss: 0.6145\n",
      "Epoch 6/25\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 595ms/step - accuracy: 0.6000 - loss: 0.6763 - val_accuracy: 1.0000 - val_loss: 0.4432\n",
      "Epoch 7/25\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 603ms/step - accuracy: 0.6000 - loss: 0.6764 - val_accuracy: 1.0000 - val_loss: 0.4104\n",
      "Epoch 8/25\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 646ms/step - accuracy: 0.6000 - loss: 0.6759 - val_accuracy: 1.0000 - val_loss: 0.4842\n",
      "Epoch 9/25\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 609ms/step - accuracy: 0.6000 - loss: 0.6669 - val_accuracy: 1.0000 - val_loss: 0.5956\n",
      "Epoch 10/25\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 590ms/step - accuracy: 0.6000 - loss: 0.6756 - val_accuracy: 1.0000 - val_loss: 0.6209\n",
      "Epoch 11/25\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 618ms/step - accuracy: 0.6000 - loss: 0.6755 - val_accuracy: 1.0000 - val_loss: 0.5421\n",
      "Epoch 12/25\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 595ms/step - accuracy: 0.6000 - loss: 0.6739 - val_accuracy: 1.0000 - val_loss: 0.4500\n",
      "Epoch 13/25\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 593ms/step - accuracy: 0.6000 - loss: 0.6743 - val_accuracy: 1.0000 - val_loss: 0.4226\n",
      "Epoch 14/25\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 620ms/step - accuracy: 0.6000 - loss: 0.6667 - val_accuracy: 1.0000 - val_loss: 0.4645\n",
      "Epoch 15/25\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 796ms/step - accuracy: 0.6000 - loss: 0.6771 - val_accuracy: 1.0000 - val_loss: 0.5408\n",
      "Epoch 16/25\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 612ms/step - accuracy: 0.6000 - loss: 0.6742 - val_accuracy: 1.0000 - val_loss: 0.5878\n",
      "Epoch 17/25\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 615ms/step - accuracy: 0.6000 - loss: 0.6781 - val_accuracy: 1.0000 - val_loss: 0.5526\n",
      "Epoch 18/25\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 622ms/step - accuracy: 0.6000 - loss: 0.6675 - val_accuracy: 1.0000 - val_loss: 0.4803\n",
      "Epoch 19/25\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 766ms/step - accuracy: 0.6000 - loss: 0.6641 - val_accuracy: 1.0000 - val_loss: 0.4329\n",
      "Epoch 20/25\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 752ms/step - accuracy: 0.6000 - loss: 0.6774 - val_accuracy: 1.0000 - val_loss: 0.4489\n",
      "Epoch 21/25\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - accuracy: 0.6000 - loss: 0.6697 - val_accuracy: 1.0000 - val_loss: 0.5077\n",
      "Epoch 22/25\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 670ms/step - accuracy: 0.6000 - loss: 0.6651 - val_accuracy: 1.0000 - val_loss: 0.5577\n",
      "Epoch 23/25\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 633ms/step - accuracy: 0.6000 - loss: 0.6646 - val_accuracy: 1.0000 - val_loss: 0.5610\n",
      "Epoch 24/25\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 636ms/step - accuracy: 0.6000 - loss: 0.6689 - val_accuracy: 1.0000 - val_loss: 0.5092\n",
      "Epoch 25/25\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 633ms/step - accuracy: 0.6000 - loss: 0.6663 - val_accuracy: 1.0000 - val_loss: 0.4557\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 1.0000 - loss: 0.4557\n",
      "Test Accuracy: 100.00\n"
     ]
    }
   ],
   "source": [
    "#Dataset splitting in Training/testing\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# EMOTHAW dataset\n",
    "def generate_sample_data(num_samples, seq_len, num_features, num_classes):\n",
    "    data = np.random.rand(num_samples, seq_len, num_features)\n",
    "    labels = np.random.randint(0, num_classes, num_samples)\n",
    "    return data, labels\n",
    "\n",
    "# Sample of EMOTHAW dataset\\n\"\n",
    "num_samples = 6\n",
    "seq_length = 800\n",
    "num_features = 56\n",
    "num_classes = 3  # Depression, Anxiety and Stress\\n\",\n",
    "\n",
    "data, labels = generate_sample_data(num_samples, seq_length, num_features, num_classes)\n",
    "\n",
    "# Split the data into training and testing sets\\n\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=1, random_state=42)\n",
    "\"\\n\",\n",
    "# Build and compile the model\\n\n",
    "num_blocks = 3\n",
    "embed_dim = 64\n",
    "num_heads = 4\n",
    "ff_dim = 128\n",
    "learning_rate = 0.001\n",
    "\n",
    "transformer_model = TransformerClassifier(num_classes, embed_dim, num_heads, ff_dim, num_blocks)\n",
    "transformer_model.compile(optimizer=Adam(learning_rate), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Training the model\n",
    "epochs = 25\n",
    "batch_size = 32\n",
    "\n",
    "transformer_model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluating the model\n",
    "loss, accuracy = transformer_model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a46a3252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-13 16:12:00,570 - 5 out of the last 35 calls to <function TensorFlowTrainer.make_train_function.<locals>.one_step_on_iterator at 0x00000252DA003A60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step - accuracy: 0.4000 - loss: 1.1108\n",
      "Epoch 2/3\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 505ms/step - accuracy: 0.6000 - loss: 0.9247\n",
      "Epoch 3/3\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 492ms/step - accuracy: 0.6000 - loss: 0.7889\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "directory = './models/'\n",
    "model_path = os.path.join(directory, 'Emotion-detection.model.keras')\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(directory, exist_ok=True)\n",
    "# Modify the model to work with sequence data\n",
    "model = tf.keras.Sequential()\n",
    "\n",
    "# LSTM layer expects 3D input (batch_size, timesteps, features)\n",
    "model.add(LSTM(128, input_shape=(800, 56), return_sequences=True))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=3)\n",
    "# Save the model\n",
    "model.save(model_path)\n",
    "\n",
    "# Compile the model\\n\",\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a522e0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d16711c9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [78784, 6]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[54], line 27\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Splitting the data into training and testing sets\u001b[39;00m\n\u001b[0;32m     26\u001b[0m features \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate((time_domain_features, frequency_domain_features, statistical_features), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 27\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Defining the model parameters\u001b[39;00m\n\u001b[0;32m     30\u001b[0m num_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\JM\\Desktop\\Validation Tool\\venv\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\JM\\Desktop\\Validation Tool\\venv\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:2782\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[1;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[0;32m   2779\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_arrays \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   2780\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAt least one array required as input\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 2782\u001b[0m arrays \u001b[38;5;241m=\u001b[39m \u001b[43mindexable\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43marrays\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2784\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m _num_samples(arrays[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m   2785\u001b[0m n_train, n_test \u001b[38;5;241m=\u001b[39m _validate_shuffle_split(\n\u001b[0;32m   2786\u001b[0m     n_samples, test_size, train_size, default_test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.25\u001b[39m\n\u001b[0;32m   2787\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\JM\\Desktop\\Validation Tool\\venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:514\u001b[0m, in \u001b[0;36mindexable\u001b[1;34m(*iterables)\u001b[0m\n\u001b[0;32m    484\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Make arrays indexable for cross-validation.\u001b[39;00m\n\u001b[0;32m    485\u001b[0m \n\u001b[0;32m    486\u001b[0m \u001b[38;5;124;03mChecks consistent length, passes through None, and ensures that everything\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    510\u001b[0m \u001b[38;5;124;03m[[1, 2, 3], array([2, 3, 4]), None, <...Sparse...dtype 'int64'...shape (3, 1)>]\u001b[39;00m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    513\u001b[0m result \u001b[38;5;241m=\u001b[39m [_make_indexable(X) \u001b[38;5;28;01mfor\u001b[39;00m X \u001b[38;5;129;01min\u001b[39;00m iterables]\n\u001b[1;32m--> 514\u001b[0m \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\JM\\Desktop\\Validation Tool\\venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:457\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    455\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[0;32m    456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 457\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    458\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    459\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[0;32m    460\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [78784, 6]"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import layers, Model\n",
    "\n",
    "class TransformerClassifier(Model):\n",
    "    def __init__(self, num_classes, embed_dim, num_heads, ff_dim, num_layers):\n",
    "        super(TransformerClassifier, self).__init__()\n",
    "        self.dense_input = layers.Dense(embed_dim)  # Add this layer to project input features\n",
    "        self.transformer_blocks = [\n",
    "            TransformerBlock(embed_dim, num_heads, ff_dim) for _ in range(num_layers)\n",
    "        ]\n",
    "        self.global_average_pooling = layers.GlobalAveragePooling1D()\n",
    "        self.dense = layers.Dense(num_classes, activation='softmax')\n",
    "\n",
    "    def call(self, inputs, training=False):  # Add training parameter\n",
    "        x = self.dense_input(inputs)  # Project inputs to the embedding dimension\n",
    "        for transformer in self.transformer_blocks:\n",
    "            x = transformer(x, training=training)  # Pass training to the transformer\n",
    "        x = self.global_average_pooling(x)\n",
    "        return self.dense(x)\n",
    "\n",
    "# Splitting the data into training and testing sets\n",
    "features = np.concatenate((time_domain_features, frequency_domain_features, statistical_features), axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Defining the model parameters\n",
    "num_classes = 3\n",
    "embed_dim = 128\n",
    "num_heads = 4\n",
    "ff_dim = 128\n",
    "num_layers = 2\n",
    "\n",
    "# Creating the Transformer model\n",
    "model = TransformerClassifier(num_classes, embed_dim, num_heads, ff_dim, num_layers)\n",
    "\n",
    "# Compiling the model\n",
    "model.compile(optimizer=Adam(clipnorm=1.0), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Training the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Evaluating the model\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d1263098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 396ms/step\n",
      "Predicted emotion: Depression\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Function to load and preprocess a single .svc file for prediction\n",
    "def load_single_svc_file(file_path, expected_timesteps=800, num_features=56):\n",
    "    try:\n",
    "        # Load the file and process the lines\n",
    "        with open(file_path, 'r') as f:\n",
    "            lines = [line.strip() for line in f.readlines() if line.strip()]  # Strip whitespace and ignore empty lines\n",
    "\n",
    "        # Process lines into a DataFrame by splitting by whitespace\n",
    "        df = pd.DataFrame([line.split() for line in lines])  # Split by whitespace\n",
    "\n",
    "        if df.empty:\n",
    "            raise ValueError(f\"{file_path} is empty after processing.\")\n",
    "\n",
    "        # Extract the feature data from the rows starting from the second row\n",
    "        data = df.iloc[1:, :].values  # All columns are features\n",
    "\n",
    "        # Convert to a float NumPy array\n",
    "        data = np.array(data, dtype=float)\n",
    "\n",
    "        # Check if we need to pad or truncate data to match (800 timesteps, 56 features)\n",
    "        if data.shape[0] > expected_timesteps:\n",
    "            # If more timesteps, truncate the excess\n",
    "            data = data[:expected_timesteps, :]\n",
    "        elif data.shape[0] < expected_timesteps:\n",
    "            # If fewer timesteps, pad with zeros\n",
    "            padding = np.zeros((expected_timesteps - data.shape[0], num_features))\n",
    "            data = np.vstack((data, padding))\n",
    "\n",
    "        # Ensure that the number of features matches the expected number (56)\n",
    "        if data.shape[1] > num_features:\n",
    "            # Truncate excess features\n",
    "            data = data[:, :num_features]\n",
    "        elif data.shape[1] < num_features:\n",
    "            # Pad missing features with zeros\n",
    "            padding = np.zeros((expected_timesteps, num_features - data.shape[1]))\n",
    "            data = np.hstack((data, padding))\n",
    "\n",
    "        # Return the reshaped data\n",
    "        return data\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Path to the .svc file to predict\n",
    "file_path = '../test/samplefew/2_hw00004 (2).svc'\n",
    "\n",
    "# Load the saved model\n",
    "model = tf.keras.models.load_model('./models/Emotion-detection.model.keras')\n",
    "\n",
    "# Load and preprocess the .svc file\n",
    "data = load_single_svc_file(file_path)\n",
    "\n",
    "# Reshape the data for prediction (1 sample, 800 timesteps, 56 features)\n",
    "if data is not None:\n",
    "    data = data.reshape((1, 800, 56))  # Reshape to match the input shape expected by the model\n",
    "\n",
    "    # Predict the class probabilities\n",
    "    predictions = model.predict(data)\n",
    "\n",
    "    # Get the predicted class (index of the highest probability)\n",
    "    predicted_class = np.argmax(predictions, axis=1)\n",
    "\n",
    "    # Output the predicted class\n",
    "    class_labels = ['Depression', 'Anxiety', 'Stress']\n",
    "    print(f\"Predicted emotion: {class_labels[predicted_class[0]]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ba062a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 1 2 2 2 1]\n"
     ]
    }
   ],
   "source": [
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "54302da5",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "load_single_svc_file() missing 2 required positional arguments: 'expected_timesteps' and 'num_features'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[97], line 57\u001b[0m\n\u001b[0;32m     54\u001b[0m model \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mload_model(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./models/Emotion-detection.model.keras\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# Load and preprocess the .svc file\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mload_single_svc_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# Reshape the data for prediction (1 sample, 800 timesteps, 56 features)\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mTypeError\u001b[0m: load_single_svc_file() missing 2 required positional arguments: 'expected_timesteps' and 'num_features'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Function to load and preprocess a single .svc file for prediction\n",
    "def load_single_svc_file(file_path, expected_timesteps=800, num_features=56):\n",
    "    try:\n",
    "        # Load the file and process the lines\n",
    "        with open(file_path, 'r') as f:\n",
    "            lines = [line.strip() for line in f.readlines() if line.strip()]  # Strip whitespace and ignore empty lines\n",
    "\n",
    "        # Process lines into a DataFrame by splitting by whitespace\n",
    "        df = pd.DataFrame([line.split() for line in lines])  # Split by whitespace\n",
    "\n",
    "        if df.empty:\n",
    "            raise ValueError(f\"{file_path} is empty after processing.\")\n",
    "\n",
    "        # Extract the feature data from the rows starting from the second row\n",
    "        data = df.iloc[1:, :].values  # All columns are features\n",
    "\n",
    "        # Convert to a float NumPy array\n",
    "        data = np.array(data, dtype=float)\n",
    "\n",
    "        # Check if we need to pad or truncate data to match (800 timesteps, 56 features)\n",
    "        if data.shape[0] > expected_timesteps:\n",
    "            # If more timesteps, truncate the excess\n",
    "            data = data[:expected_timesteps, :]\n",
    "        elif data.shape[0] < expected_timesteps:\n",
    "            # If fewer timesteps, pad with zeros\n",
    "            padding = np.zeros((expected_timesteps - data.shape[0], num_features))\n",
    "            data = np.vstack((data, padding))\n",
    "\n",
    "        # Ensure that the number of features matches the expected number (56)\n",
    "        if data.shape[1] > num_features:\n",
    "            # Truncate excess features\n",
    "            data = data[:, :num_features]\n",
    "        elif data.shape[1] < num_features:\n",
    "            # Pad missing features with zeros\n",
    "            padding = np.zeros((expected_timesteps, num_features - data.shape[1]))\n",
    "            data = np.hstack((data, padding))\n",
    "\n",
    "        # Return the reshaped data\n",
    "        return data\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Path to the .svc file to predict\n",
    "file_path = '../test/samplefew/0_hw00001(2).svc'\n",
    "\n",
    "# Load the saved model\n",
    "model = tf.keras.models.load_model('./models/Emotion-detection.model.keras')\n",
    "\n",
    "# Load and preprocess the .svc file\n",
    "data = load_single_svc_file(file_path)\n",
    "\n",
    "# Reshape the data for prediction (1 sample, 800 timesteps, 56 features)\n",
    "if data is not None:\n",
    "    data = data.reshape((1, 800, 56))  # Reshape to match the input shape expected by the model\n",
    "\n",
    "    # Predict the class probabilities\n",
    "    predictions = model.predict(data)\n",
    "\n",
    "    # Get the predicted class (index of the highest probability)\n",
    "    predicted_class = np.argmax(predictions, axis=1)\n",
    "\n",
    "    # Output the predicted class\n",
    "    class_labels = ['Depression', 'Anxiety', 'Stress']\n",
    "    print(f\"Predicted emotion: {class_labels[predicted_class[0]]}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
