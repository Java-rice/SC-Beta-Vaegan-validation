{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69101bf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: ../test/new_sample\\0_hw00001(2).svc\n",
      "Shape of data from file ../test/new_sample\\0_hw00001(2).svc: (3444, 7)\n",
      "Processing file: ../test/new_sample\\0_hw00001(2)21.svc\n",
      "Shape of data from file ../test/new_sample\\0_hw00001(2)21.svc: (2086, 7)\n",
      "Processing file: ../test/new_sample\\0_hw00003(2).svc\n",
      "Shape of data from file ../test/new_sample\\0_hw00003(2).svc: (3959, 7)\n",
      "Processing file: ../test/new_sample\\0_hw00003.svc\n",
      "Shape of data from file ../test/new_sample\\0_hw00003.svc: (3102, 7)\n",
      "Processing file: ../test/new_sample\\1_hw00002(2).svc\n",
      "Shape of data from file ../test/new_sample\\1_hw00002(2).svc: (3221, 7)\n",
      "Processing file: ../test/new_sample\\1_hw00003(2).svc\n",
      "Shape of data from file ../test/new_sample\\1_hw00003(2).svc: (4401, 7)\n",
      "Processing file: ../test/new_sample\\1_hw00003.svc\n",
      "Shape of data from file ../test/new_sample\\1_hw00003.svc: (4276, 7)\n",
      "Processing file: ../test/new_sample\\1_hw00004(2).svc\n",
      "Shape of data from file ../test/new_sample\\1_hw00004(2).svc: (2315, 7)\n",
      "Processing file: ../test/new_sample\\2_hw00004 (2).svc\n",
      "Shape of data from file ../test/new_sample\\2_hw00004 (2).svc: (1111, 7)\n",
      "Processing file: ../test/new_sample\\2_hw00004.svc\n",
      "Shape of data from file ../test/new_sample\\2_hw00004.svc: (1951, 7)\n",
      "Processing file: ../test/new_sample\\2_hw00005(2).svc\n",
      "Shape of data from file ../test/new_sample\\2_hw00005(2).svc: (1852, 7)\n",
      "Processing file: ../test/new_sample\\2_hw00006.svc\n",
      "Shape of data from file ../test/new_sample\\2_hw00006.svc: (3178, 7)\n",
      "Shapes of all_data before concatenation:\n",
      "Data array 0 shape: (3444, 7)\n",
      "Data array 1 shape: (2086, 7)\n",
      "Data array 2 shape: (3959, 7)\n",
      "Data array 3 shape: (3102, 7)\n",
      "Data array 4 shape: (3221, 7)\n",
      "Data array 5 shape: (4401, 7)\n",
      "Data array 6 shape: (4276, 7)\n",
      "Data array 7 shape: (2315, 7)\n",
      "Data array 8 shape: (1111, 7)\n",
      "Data array 9 shape: (1951, 7)\n",
      "Data array 10 shape: (1852, 7)\n",
      "Data array 11 shape: (3178, 7)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "\n",
    "def load_emothaw_data(directory_path):\n",
    "    all_data = []\n",
    "    all_labels = []\n",
    "\n",
    "    # Use glob to find all SVC files in the specified directory\n",
    "    file_paths = glob.glob(os.path.join(directory_path, \"*.svc\"))\n",
    "    if not file_paths:\n",
    "        raise ValueError(\"No files found in the specified directory\")\n",
    "\n",
    "    for file_path in file_paths:\n",
    "        print(f\"Processing file: {file_path}\")\n",
    "        try:\n",
    "            # Extract the label from the filename\n",
    "            label = int(os.path.basename(file_path).split('_')[0])  # Assuming the label is the part before the first '_'\n",
    "\n",
    "            # Load the file and remove trailing spaces\n",
    "            with open(file_path, 'r') as f:\n",
    "                lines = [line.strip() for line in f.readlines() if line.strip()]  # Strip whitespace and ignore empty lines\n",
    "\n",
    "            # Process lines into a DataFrame by splitting by whitespace\n",
    "            df = pd.DataFrame([line.split() for line in lines])  # Split by whitespace\n",
    "\n",
    "            if df.empty:\n",
    "                print(f\"Warning: {file_path} is empty after processing. Skipping file.\")\n",
    "                continue\n",
    "\n",
    "            # The first row gives the number of rows starting from the second row\n",
    "            total_rows = int(df.iloc[0, 0])  # Number of sequences\n",
    "\n",
    "            # Extract the feature data from the rows starting from the second row\n",
    "            data = df.iloc[1:, :].values  # All columns are features\n",
    "\n",
    "            if data.shape[0] < total_rows:\n",
    "                raise ValueError(f\"Insufficient data: expected {total_rows}, but found {data.shape[0]}\")\n",
    "\n",
    "            # Reshape the data to ensure each row has 7 values\n",
    "            reshaped_data = []\n",
    "            for row in data:\n",
    "                # Ensure each row has exactly 7 columns\n",
    "                if len(row) == 7:\n",
    "                    reshaped_data.append(row)\n",
    "                else:\n",
    "                    print(f\"Warning: Row does not have 7 values, skipping: {row}\")\n",
    "\n",
    "            # Convert the list of rows to a NumPy array only if not empty\n",
    "            if reshaped_data:\n",
    "                reshaped_data = np.array(reshaped_data, dtype=float)  # Ensure float type\n",
    "\n",
    "                # Check that reshaped_data has enough rows\n",
    "                if reshaped_data.shape[0] < total_rows:\n",
    "                    raise ValueError(f\"Insufficient data: expected {total_rows}, but found {reshaped_data.shape[0]}\")\n",
    "\n",
    "                # Append the reshaped data and labels to the lists\n",
    "                all_data.append(reshaped_data)\n",
    "                all_labels.append([label] * reshaped_data.shape[0])\n",
    "\n",
    "                # Print the shape of reshaped data for debugging\n",
    "                print(f\"Shape of data from file {file_path}: {reshaped_data.shape}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {file_path}: {e}\")\n",
    "            continue\n",
    "\n",
    "    if not all_data or not all_labels:\n",
    "        raise ValueError(\"No valid data found in the directory\")\n",
    "\n",
    "    # Display shapes of individual entries in all_data\n",
    "    print(\"Shapes of all_data before concatenation:\")\n",
    "    for i, data_array in enumerate(all_data):\n",
    "        print(f\"Data array {i} shape: {data_array.shape}\")\n",
    "\n",
    "    # Attempt to concatenate all data and labels from the files\n",
    "    try:\n",
    "        all_data = np.concatenate(all_data, axis=0)  # Concatenate along the sample axis\n",
    "    except Exception as e:\n",
    "        print(f\"Error during concatenation: {e}\")\n",
    "\n",
    "    all_labels = np.concatenate(all_labels, axis=0)  # Concatenate labels\n",
    "\n",
    "    return all_data, all_labels\n",
    "\n",
    "# Example usage\n",
    "data, labels = load_emothaw_data(r'../test/new_sample')  # Use raw string for paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3cda98d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 2 2 2]\n"
     ]
    }
   ],
   "source": [
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f430eaf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-14 20:18:09,686 - Starting time-domain feature extraction\n",
      "Extracting features: 100%|██████████| 34896/34896 [02:37<00:00, 221.20sample/s]\n",
      "2024-10-14 20:20:47,450 - Feature extraction completed in 0:02:37.763930\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# Function to extract time-domain features with progress bar and logging\n",
    "def extract_time_domain_features(data):\n",
    "    logger.info(\"Starting time-domain feature extraction\")\n",
    "    start_time = datetime.now()\n",
    "\n",
    "    time_features = []\n",
    "    \n",
    "    # Use tqdm to create a progress bar\n",
    "    for sample in tqdm(data, desc=\"Extracting features\", unit=\"sample\"):\n",
    "        sample_features = []\n",
    "        for feature in sample.T:  # Assuming the features are along the last axis\n",
    "            feature_stats = [\n",
    "                np.mean(feature),\n",
    "                np.std(feature),\n",
    "                np.min(feature),\n",
    "                np.max(feature),\n",
    "                np.percentile(feature, 25),\n",
    "                np.percentile(feature, 50),\n",
    "                np.percentile(feature, 75),\n",
    "            ]\n",
    "            sample_features.extend(feature_stats)  # Append the computed stats to sample_features\n",
    "        time_features.append(sample_features)  # Append the features for the sample\n",
    "\n",
    "    end_time = datetime.now()\n",
    "    elapsed_time = end_time - start_time\n",
    "    logger.info(f\"Feature extraction completed in {elapsed_time}\")\n",
    "\n",
    "    return np.array(time_features)\n",
    "\n",
    "# Assuming 'data' is your input 3D array (samples, sequence length, features)\n",
    "time_domain_features = extract_time_domain_features(data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad8fc6f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.0962e+04 0.0000e+00 5.0962e+04 ... 1.5000e+01 1.5000e+01 1.5000e+01]\n",
      " [5.0962e+04 0.0000e+00 5.0962e+04 ... 4.5000e+01 4.5000e+01 4.5000e+01]\n",
      " [5.0962e+04 0.0000e+00 5.0962e+04 ... 7.5000e+01 7.5000e+01 7.5000e+01]\n",
      " ...\n",
      " [1.9867e+04 0.0000e+00 1.9867e+04 ... 5.1500e+02 5.1500e+02 5.1500e+02]\n",
      " [1.9860e+04 0.0000e+00 1.9860e+04 ... 3.6800e+02 3.6800e+02 3.6800e+02]\n",
      " [1.9860e+04 0.0000e+00 1.9860e+04 ... 1.1800e+02 1.1800e+02 1.1800e+02]]\n"
     ]
    }
   ],
   "source": [
    "print(time_domain_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "830915a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-14 20:20:48,019 - Starting frequency-domain feature extraction\n",
      "Extracting frequency features: 100%|██████████| 34896/34896 [00:01<00:00, 23357.94sample/s]\n",
      "2024-10-14 20:20:49,520 - Frequency-domain feature extraction completed in 0:00:01.498497\n",
      "2024-10-14 20:20:49,545 - Data: [[5.0962000e+04 3.4188000e+04 1.6718871e+07 ... 1.9500000e+03\n",
      "  6.2000000e+02 1.5000000e+01]\n",
      " [5.0962000e+04 3.4188000e+04 1.6718878e+07 ... 1.9500000e+03\n",
      "  6.2000000e+02 4.5000000e+01]\n",
      " [5.0962000e+04 3.4188000e+04 1.6718886e+07 ... 1.9500000e+03\n",
      "  6.2000000e+02 7.5000000e+01]\n",
      " ...\n",
      " [1.9867000e+04 1.0467000e+04 1.6267750e+06 ... 1.9100000e+03\n",
      "  5.8000000e+02 5.1500000e+02]\n",
      " [1.9860000e+04 1.0461000e+04 1.6267820e+06 ... 1.9100000e+03\n",
      "  5.8000000e+02 3.6800000e+02]\n",
      " [1.9860000e+04 1.0457000e+04 1.6267900e+06 ... 1.9100000e+03\n",
      "  5.8000000e+02 1.1800000e+02]]\n",
      "2024-10-14 20:20:49,547 - Frequency Domain Features: [[0.00000000e+00 1.17032471e+08]\n",
      " [0.00000000e+00 1.17032520e+08]\n",
      " [0.00000000e+00 1.17032576e+08]\n",
      " ...\n",
      " [0.00000000e+00 1.13878894e+07]\n",
      " [0.00000000e+00 1.13879380e+07]\n",
      " [0.00000000e+00 1.13879941e+07]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# Extracting frequency domain features using FFT with progress bar and logging\n",
    "def extract_frequency_domain_features(data):\n",
    "    logger.info(\"Starting frequency-domain feature extraction\")\n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    freq_features = []\n",
    "    \n",
    "    # Ensure the data has the correct dimensions\n",
    "    if len(data.shape) != 2:\n",
    "        raise ValueError(f\"Expected data with 2 dimensions (samples, features), but got {data.shape}\")\n",
    "    \n",
    "    # Use tqdm to create a progress bar\n",
    "    for sample in tqdm(data, desc=\"Extracting frequency features\", unit=\"sample\"):\n",
    "        sample_features = []\n",
    "        \n",
    "        # Check if the sample is a 1D array\n",
    "        if len(sample.shape) != 1:\n",
    "            raise ValueError(f\"Expected each sample to be 1D, but got {sample.shape}\")\n",
    "\n",
    "        # Apply FFT to the sample (which is already 1D, hence no need for .T)\n",
    "        freq_feature = np.fft.fft(sample)\n",
    "        freq_magnitude = np.abs(freq_feature)\n",
    "        dominant_freq = np.argmax(freq_magnitude)\n",
    "        freq_energy = np.sum(freq_magnitude)\n",
    "        \n",
    "        sample_features.extend([dominant_freq, freq_energy])\n",
    "        freq_features.append(sample_features)\n",
    "    \n",
    "    end_time = datetime.now()\n",
    "    elapsed_time = end_time - start_time\n",
    "    logger.info(f\"Frequency-domain feature extraction completed in {elapsed_time}\")\n",
    "\n",
    "    return np.array(freq_features)\n",
    "\n",
    "# Example usage: Assuming `data` is a 2D array (samples, features)\n",
    "# data = np.random.rand(100, 50)  # Example data\n",
    "frequency_domain_features = extract_frequency_domain_features(data)\n",
    "\n",
    "# Log the output\n",
    "logger.info(f\"Data: {data}\")\n",
    "logger.info(f\"Frequency Domain Features: {frequency_domain_features}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19706a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(frequency_domain_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a7bf958",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-14 20:21:02,567 - Starting statistical feature extraction\n",
      "Extracting statistical features: 100%|██████████| 34896/34896 [01:37<00:00, 356.92sample/s]\n",
      "2024-10-14 20:22:40,342 - Statistical feature extraction completed in 0:01:37.773200\n",
      "2024-10-14 20:22:40,506 - Data: [[5.0962000e+04 3.4188000e+04 1.6718871e+07 ... 1.9500000e+03\n",
      "  6.2000000e+02 1.5000000e+01]\n",
      " [5.0962000e+04 3.4188000e+04 1.6718878e+07 ... 1.9500000e+03\n",
      "  6.2000000e+02 4.5000000e+01]\n",
      " [5.0962000e+04 3.4188000e+04 1.6718886e+07 ... 1.9500000e+03\n",
      "  6.2000000e+02 7.5000000e+01]\n",
      " ...\n",
      " [1.9867000e+04 1.0467000e+04 1.6267750e+06 ... 1.9100000e+03\n",
      "  5.8000000e+02 5.1500000e+02]\n",
      " [1.9860000e+04 1.0461000e+04 1.6267820e+06 ... 1.9100000e+03\n",
      "  5.8000000e+02 3.6800000e+02]\n",
      " [1.9860000e+04 1.0457000e+04 1.6267900e+06 ... 1.9100000e+03\n",
      "  5.8000000e+02 1.1800000e+02]]\n",
      "2024-10-14 20:22:40,508 - Statistical Features: [[5.09620000e+04 5.09620000e+04 0.00000000e+00 ... 0.00000000e+00\n",
      "  2.04119691e+00 2.16656956e+00]\n",
      " [5.09620000e+04 5.09620000e+04 0.00000000e+00 ... 0.00000000e+00\n",
      "  2.04119692e+00 2.16656959e+00]\n",
      " [5.09620000e+04 5.09620000e+04 0.00000000e+00 ... 0.00000000e+00\n",
      "  2.04119694e+00 2.16656962e+00]\n",
      " ...\n",
      " [1.98670000e+04 1.98670000e+04 0.00000000e+00 ... 0.00000000e+00\n",
      "  2.04062853e+00 2.16532455e+00]\n",
      " [1.98600000e+04 1.98600000e+04 0.00000000e+00 ... 0.00000000e+00\n",
      "  2.04062621e+00 2.16531946e+00]\n",
      " [1.98600000e+04 1.98600000e+04 0.00000000e+00 ... 0.00000000e+00\n",
      "  2.04062133e+00 2.16530878e+00]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import skew, kurtosis\n",
    "from datetime import datetime\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# Function to extract statistical features with progress bar and logging\n",
    "def extract_statistical_features(data):\n",
    "    logger.info(\"Starting statistical feature extraction\")\n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    statistical_features = []\n",
    "    \n",
    "    for sample in tqdm(data, desc=\"Extracting statistical features\", unit=\"sample\"):\n",
    "        sample_features = []\n",
    "        skew_value = skew(sample.T)\n",
    "        kurtosis_value = kurtosis(sample.T)\n",
    "        for feature in sample.T:\n",
    "            mean = np.mean(feature)\n",
    "            median = np.median(feature)\n",
    "            variance = np.var(feature)\n",
    "            feature_stats = [mean, median, variance, skew_value, kurtosis_value]\n",
    "            sample_features.extend(feature_stats)\n",
    "        statistical_features.append(sample_features)\n",
    "    \n",
    "    end_time = datetime.now()\n",
    "    elapsed_time = end_time - start_time\n",
    "    logger.info(f\"Statistical feature extraction completed in {elapsed_time}\")\n",
    "    \n",
    "    return np.array(statistical_features)\n",
    "\n",
    "# Example usage: Assuming 'data' is a 3D array (samples, sequence length, features)\n",
    "statistical_features = extract_statistical_features(data)\n",
    "\n",
    "# Log the output\n",
    "logger.info(f\"Data: {data}\")\n",
    "logger.info(f\"Statistical Features: {statistical_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "929a5094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.0962e+04 0.0000e+00 5.0962e+04 ... 1.5000e+01 1.5000e+01 1.5000e+01]\n",
      " [5.0962e+04 0.0000e+00 5.0962e+04 ... 4.5000e+01 4.5000e+01 4.5000e+01]\n",
      " [5.0962e+04 0.0000e+00 5.0962e+04 ... 7.5000e+01 7.5000e+01 7.5000e+01]\n",
      " ...\n",
      " [1.9867e+04 0.0000e+00 1.9867e+04 ... 5.1500e+02 5.1500e+02 5.1500e+02]\n",
      " [1.9860e+04 0.0000e+00 1.9860e+04 ... 3.6800e+02 3.6800e+02 3.6800e+02]\n",
      " [1.9860e+04 0.0000e+00 1.9860e+04 ... 1.1800e+02 1.1800e+02 1.1800e+02]]\n",
      "[[0.00000000e+00 1.17032471e+08]\n",
      " [0.00000000e+00 1.17032520e+08]\n",
      " [0.00000000e+00 1.17032576e+08]\n",
      " ...\n",
      " [0.00000000e+00 1.13878894e+07]\n",
      " [0.00000000e+00 1.13879380e+07]\n",
      " [0.00000000e+00 1.13879941e+07]]\n",
      "[[5.09620000e+04 5.09620000e+04 0.00000000e+00 ... 0.00000000e+00\n",
      "  2.04119691e+00 2.16656956e+00]\n",
      " [5.09620000e+04 5.09620000e+04 0.00000000e+00 ... 0.00000000e+00\n",
      "  2.04119692e+00 2.16656959e+00]\n",
      " [5.09620000e+04 5.09620000e+04 0.00000000e+00 ... 0.00000000e+00\n",
      "  2.04119694e+00 2.16656962e+00]\n",
      " ...\n",
      " [1.98670000e+04 1.98670000e+04 0.00000000e+00 ... 0.00000000e+00\n",
      "  2.04062853e+00 2.16532455e+00]\n",
      " [1.98600000e+04 1.98600000e+04 0.00000000e+00 ... 0.00000000e+00\n",
      "  2.04062621e+00 2.16531946e+00]\n",
      " [1.98600000e+04 1.98600000e+04 0.00000000e+00 ... 0.00000000e+00\n",
      "  2.04062133e+00 2.16530878e+00]]\n"
     ]
    }
   ],
   "source": [
    "print(time_domain_features)\n",
    "print(frequency_domain_features)\n",
    "print(statistical_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e9455a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying Attention-based Transformer Model\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "\n",
    "# Applying Attention-based Transformer Model\n",
    "class MultiHeadSelfAttention(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        if embed_dim % num_heads != 0:\n",
    "            raise ValueError(f\"embedding dimension = {embed_dim} should be divisible by number of heads = {num_heads}\")\n",
    "        self.projection_dim = embed_dim // num_heads\n",
    "        self.query_dense = layers.Dense(embed_dim)\n",
    "        self.key_dense = layers.Dense(embed_dim)\n",
    "        self.value_dense = layers.Dense(embed_dim)\n",
    "        self.combine_heads = layers.Dense(embed_dim)\n",
    "\n",
    "    def attention(self, query, key, value):\n",
    "        score = tf.matmul(query, key, transpose_b=True)\n",
    "        dim_key = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "        scaled_score = score / tf.math.sqrt(dim_key)\n",
    "        weights = tf.nn.softmax(scaled_score, axis=-1)\n",
    "        output = tf.matmul(weights, value)\n",
    "        return output, weights\n",
    "\n",
    "    def separate_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        query = self.query_dense(inputs)\n",
    "        key = self.key_dense(inputs)\n",
    "        value = self.value_dense(inputs)\n",
    "        query = self.separate_heads(query, batch_size)\n",
    "        key = self.separate_heads(key, batch_size)\n",
    "        value = self.separate_heads(value, batch_size)\n",
    "        attention, weights = self.attention(query, key, value)\n",
    "        attention = tf.transpose(attention, perm=[0, 2, 1, 3])\n",
    "        concat_attention = tf.reshape(attention, (batch_size, -1, self.embed_dim))\n",
    "        output = self.combine_heads(concat_attention)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "63464e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = MultiHeadSelfAttention(embed_dim, num_heads)\n",
    "        self.ffn = tf.keras.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim)]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5ef508e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m698/698\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 38ms/step - accuracy: 0.4660 - loss: 1.0264 - val_accuracy: 0.7876 - val_loss: 0.4824\n",
      "Epoch 2/10\n",
      "\u001b[1m698/698\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 38ms/step - accuracy: 0.7826 - loss: 0.4409 - val_accuracy: 0.8218 - val_loss: 0.3709\n",
      "Epoch 3/10\n",
      "\u001b[1m698/698\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 38ms/step - accuracy: 0.8176 - loss: 0.3728 - val_accuracy: 0.8653 - val_loss: 0.3023\n",
      "Epoch 4/10\n",
      "\u001b[1m698/698\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 37ms/step - accuracy: 0.8467 - loss: 0.3380 - val_accuracy: 0.8899 - val_loss: 0.2188\n",
      "Epoch 5/10\n",
      "\u001b[1m698/698\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 35ms/step - accuracy: 0.8626 - loss: 0.2945 - val_accuracy: 0.8977 - val_loss: 0.2061\n",
      "Epoch 6/10\n",
      "\u001b[1m698/698\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 41ms/step - accuracy: 0.8667 - loss: 0.2838 - val_accuracy: 0.8890 - val_loss: 0.2253\n",
      "Epoch 7/10\n",
      "\u001b[1m698/698\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 41ms/step - accuracy: 0.8565 - loss: 0.2988 - val_accuracy: 0.7690 - val_loss: 0.5127\n",
      "Epoch 8/10\n",
      "\u001b[1m698/698\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 46ms/step - accuracy: 0.8431 - loss: 0.3491 - val_accuracy: 0.8985 - val_loss: 0.2228\n",
      "Epoch 9/10\n",
      "\u001b[1m698/698\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 55ms/step - accuracy: 0.8761 - loss: 0.2537 - val_accuracy: 0.8677 - val_loss: 0.2995\n",
      "Epoch 10/10\n",
      "\u001b[1m698/698\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 44ms/step - accuracy: 0.8553 - loss: 0.3058 - val_accuracy: 0.9020 - val_loss: 0.2188\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - accuracy: 0.8967 - loss: 0.2278\n",
      "Test Loss: 0.22332967817783356, Test Accuracy: 0.8991404175758362\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import layers, Model\n",
    "\n",
    "class TransformerClassifier(Model):\n",
    "    def __init__(self, num_classes, embed_dim, num_heads, ff_dim, num_layers):\n",
    "        super(TransformerClassifier, self).__init__()\n",
    "        self.dense_input = layers.Dense(embed_dim)  # Add this layer to project input features\n",
    "        self.transformer_blocks = [\n",
    "            TransformerBlock(embed_dim, num_heads, ff_dim) for _ in range(num_layers)\n",
    "        ]\n",
    "        self.global_average_pooling = layers.GlobalAveragePooling1D()\n",
    "        self.dense = layers.Dense(num_classes, activation='softmax')\n",
    "\n",
    "    def call(self, inputs, training=False):  # Add training parameter\n",
    "        x = self.dense_input(inputs)  # Project inputs to the embedding dimension\n",
    "        for transformer in self.transformer_blocks:\n",
    "            x = transformer(x, training=training)  # Pass training to the transformer\n",
    "        x = self.global_average_pooling(x)\n",
    "        return self.dense(x)\n",
    "\n",
    "# Splitting the data into training and testing sets\n",
    "features = np.concatenate((time_domain_features, frequency_domain_features, statistical_features), axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Defining the model parameters\n",
    "num_classes = 3\n",
    "embed_dim = 128\n",
    "num_heads = 4\n",
    "ff_dim = 128\n",
    "num_layers = 2\n",
    "\n",
    "# Creating the Transformer model\n",
    "model = TransformerClassifier(num_classes, embed_dim, num_heads, ff_dim, num_layers)\n",
    "\n",
    "# Compiling the model\n",
    "model.compile(optimizer=Adam(clipnorm=1.0), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Training the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Evaluating the model\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6a69d11a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-14 21:34:28,209 - You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "# After training the model, save it\n",
    "model.save('testtransformer_classifier_model.h5')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ae26d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fe8ad576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m1091/1091\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 31ms/step - accuracy: 0.5255 - loss: 0.8931 - val_accuracy: 0.0000e+00 - val_loss: 2.6839\n",
      "Epoch 2/5\n",
      "\u001b[1m1091/1091\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 35ms/step - accuracy: 0.8384 - loss: 0.3315 - val_accuracy: 0.0000e+00 - val_loss: 3.7750\n",
      "Epoch 3/5\n",
      "\u001b[1m1091/1091\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 38ms/step - accuracy: 0.8593 - loss: 0.2831 - val_accuracy: 0.0000e+00 - val_loss: 2.2306\n",
      "Epoch 4/5\n",
      "\u001b[1m1091/1091\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 31ms/step - accuracy: 0.8610 - loss: 0.2810 - val_accuracy: 1.0000 - val_loss: 0.4859\n",
      "Epoch 5/5\n",
      "\u001b[1m1091/1091\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 35ms/step - accuracy: 0.8204 - loss: 0.3756 - val_accuracy: 1.0000 - val_loss: 0.5605\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 1.0000 - loss: 0.5605\n",
      "Test Accuracy: 100.00\n"
     ]
    }
   ],
   "source": [
    "#Dataset splitting in Training/testing\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# # EMOTHAW dataset\n",
    "# def generate_sample_data(num_samples, seq_len, num_features, num_classes):\n",
    "#     data = np.random.rand(num_samples, seq_len, num_features)\n",
    "#     labels = np.random.randint(0, num_classes, num_samples)\n",
    "#     return data, labels\n",
    "\n",
    "# # Sample of EMOTHAW dataset\\n\"\n",
    "# num_samples = 12\n",
    "# seq_length = 800\n",
    "# num_features = 56\n",
    "# num_classes = 3  # Depression, Anxiety and Stress\\n\",\n",
    "\n",
    "# data, labels = generate_sample_data(num_samples, seq_length, num_features, num_classes)\n",
    "\n",
    "# Split the data into training and testing sets\\n\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=1, random_state=42)\n",
    "\"\\n\",\n",
    "# Build and compile the model\\n\n",
    "num_blocks = 3\n",
    "embed_dim = 64\n",
    "num_heads = 4\n",
    "ff_dim = 128\n",
    "learning_rate = 0.001\n",
    "\n",
    "transformer_model = TransformerClassifier(num_classes, embed_dim, num_heads, ff_dim, num_blocks)\n",
    "transformer_model.compile(optimizer=Adam(learning_rate), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Training the model\n",
    "epochs = 5\n",
    "batch_size = 32\n",
    "\n",
    "transformer_model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluating the model\n",
    "loss, accuracy = transformer_model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a522e0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a46a3252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling Sequential.call().\n\n\u001b[1mInvalid input shape for input Tensor(\"data:0\", shape=(None, 7), dtype=float32). Expected shape (None, None, 56), but input has incompatible shape (None, 7)\u001b[0m\n\nArguments received by Sequential.call():\n  • inputs=tf.Tensor(shape=(None, 7), dtype=float32)\n  • training=True\n  • mask=None",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 28\u001b[0m\n\u001b[0;32m     25\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msparse_categorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Train the model (ensure X_train has shape (batch_size, variable_timesteps, features))\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Save the model\u001b[39;00m\n\u001b[0;32m     31\u001b[0m model\u001b[38;5;241m.\u001b[39msave(model_path)\n",
      "File \u001b[1;32mc:\\Users\\JM\\Desktop\\Validation Tool\\venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\JM\\Desktop\\Validation Tool\\venv\\Lib\\site-packages\\keras\\src\\models\\functional.py:264\u001b[0m, in \u001b[0;36mFunctional._adjust_input_rank\u001b[1;34m(self, flat_inputs)\u001b[0m\n\u001b[0;32m    262\u001b[0m             adjusted\u001b[38;5;241m.\u001b[39mappend(ops\u001b[38;5;241m.\u001b[39mexpand_dims(x, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m    263\u001b[0m             \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m--> 264\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    265\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid input shape for input \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Expected shape \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    266\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mref_shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but input has incompatible shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    267\u001b[0m     )\n\u001b[0;32m    268\u001b[0m \u001b[38;5;66;03m# Add back metadata.\u001b[39;00m\n\u001b[0;32m    269\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(flat_inputs)):\n",
      "\u001b[1;31mValueError\u001b[0m: Exception encountered when calling Sequential.call().\n\n\u001b[1mInvalid input shape for input Tensor(\"data:0\", shape=(None, 7), dtype=float32). Expected shape (None, None, 56), but input has incompatible shape (None, 7)\u001b[0m\n\nArguments received by Sequential.call():\n  • inputs=tf.Tensor(shape=(None, 7), dtype=float32)\n  • training=True\n  • mask=None"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "\n",
    "# Define the directory and model path\n",
    "directory = './models/'\n",
    "model_path = os.path.join(directory, '2Emotion-detectiontest.model.keras')\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "# Define the number of classes (adjust this according to your data)\n",
    "num_classes = 3  # Example number of classes\n",
    "\n",
    "# Modify the model to work with sequence data and adaptive input shape\n",
    "model = tf.keras.Sequential()\n",
    "\n",
    "# LSTM layer with variable timesteps and fixed features (56 features in this case)\n",
    "model.add(LSTM(128, input_shape=(None, 56), return_sequences=True))  # Adaptive timesteps\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model (ensure X_train has shape (batch_size, variable_timesteps, features))\n",
    "model.fit(X_train, y_train, epochs=3)\n",
    "\n",
    "# Save the model\n",
    "model.save(model_path)\n",
    "\n",
    "# Optionally, recompile the model (not necessary right after training)\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d16711c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m698/698\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 37ms/step - accuracy: 0.4573 - loss: 0.9943 - val_accuracy: 0.7020 - val_loss: 0.7195\n",
      "Epoch 2/10\n",
      "\u001b[1m698/698\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 33ms/step - accuracy: 0.6253 - loss: 0.7491 - val_accuracy: 0.8109 - val_loss: 0.4054\n",
      "Epoch 3/10\n",
      "\u001b[1m698/698\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 33ms/step - accuracy: 0.7817 - loss: 0.4606 - val_accuracy: 0.8044 - val_loss: 0.4162\n",
      "Epoch 4/10\n",
      "\u001b[1m698/698\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 31ms/step - accuracy: 0.8145 - loss: 0.3747 - val_accuracy: 0.8739 - val_loss: 0.2486\n",
      "Epoch 5/10\n",
      "\u001b[1m698/698\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 39ms/step - accuracy: 0.8620 - loss: 0.2717 - val_accuracy: 0.8798 - val_loss: 0.2247\n",
      "Epoch 6/10\n",
      "\u001b[1m698/698\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 36ms/step - accuracy: 0.8592 - loss: 0.2932 - val_accuracy: 0.8730 - val_loss: 0.2522\n",
      "Epoch 7/10\n",
      "\u001b[1m698/698\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 40ms/step - accuracy: 0.8813 - loss: 0.2393 - val_accuracy: 0.9020 - val_loss: 0.1872\n",
      "Epoch 8/10\n",
      "\u001b[1m698/698\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 47ms/step - accuracy: 0.8762 - loss: 0.2442 - val_accuracy: 0.8990 - val_loss: 0.1858\n",
      "Epoch 9/10\n",
      "\u001b[1m698/698\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 55ms/step - accuracy: 0.8851 - loss: 0.2179 - val_accuracy: 0.9004 - val_loss: 0.1799\n",
      "Epoch 10/10\n",
      "\u001b[1m698/698\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 41ms/step - accuracy: 0.8677 - loss: 0.2699 - val_accuracy: 0.9020 - val_loss: 0.1839\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - accuracy: 0.8967 - loss: 0.1864\n",
      "Test Loss: 0.18607497215270996, Test Accuracy: 0.8989971280097961\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import layers, Model\n",
    "\n",
    "class TransformerClassifier(tf.keras.Model):\n",
    "    def __init__(self, num_classes, embed_dim, num_heads, ff_dim, num_layers):\n",
    "        super(TransformerClassifier, self).__init__()\n",
    "        self.dense_input = layers.Dense(embed_dim)  # Add this layer to project input features\n",
    "        self.transformer_blocks = [\n",
    "            TransformerBlock(embed_dim, num_heads, ff_dim) for _ in range(num_layers)\n",
    "        ]\n",
    "        self.global_average_pooling = layers.GlobalAveragePooling1D()\n",
    "        self.dense = layers.Dense(num_classes, activation='softmax')\n",
    "\n",
    "    def call(self, inputs, training=False):  # Add training parameter\n",
    "        x = self.dense_input(inputs)  # Project inputs to the embedding dimension\n",
    "        for transformer in self.transformer_blocks:\n",
    "            x = transformer(x, training=training)  # Pass training to the transformer\n",
    "        x = self.global_average_pooling(x)\n",
    "        return self.dense(x)\n",
    "\n",
    "# Splitting the data into training and testing sets\n",
    "features = np.concatenate((time_domain_features, frequency_domain_features, statistical_features), axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Defining the model parameters\n",
    "num_classes = 3\n",
    "embed_dim = 128\n",
    "num_heads = 4\n",
    "ff_dim = 128\n",
    "num_layers = 2\n",
    "\n",
    "# Creating the Transformer model\n",
    "model = TransformerClassifier(num_classes, embed_dim, num_heads, ff_dim, num_layers)\n",
    "\n",
    "# Compiling the model\n",
    "model.compile(optimizer=Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Training the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Evaluating the model\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "990a3eea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-14 21:45:40,273 - You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "# After training the model, save it\n",
    "model.save('trytransformer_classifier_model.h5')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d1263098",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Unable to revive model from config. When overriding the `get_config()` method, make sure that the returned config contains all items used as arguments in the  constructor to <class '__main__.TransformerClassifier'>, which is the default behavior. You can override this default behavior by defining a `from_config(cls, config)` class method to specify how to create an instance of TransformerClassifier from its config.\n\nReceived config={'trainable': True, 'dtype': {'module': 'keras', 'class_name': 'DTypePolicy', 'config': {'name': 'float32'}, 'registered_name': None}}\n\nError encountered during deserialization: TransformerClassifier.__init__() got an unexpected keyword argument 'trainable'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\JM\\Desktop\\Validation Tool\\venv\\Lib\\site-packages\\keras\\src\\models\\model.py:538\u001b[0m, in \u001b[0;36mModel.from_config\u001b[1;34m(cls, config, custom_objects)\u001b[0m\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    539\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mTypeError\u001b[0m: TransformerClassifier.__init__() got an unexpected keyword argument 'trainable'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 37\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# Load the custom model\u001b[39;00m\n\u001b[0;32m     36\u001b[0m custom_objects \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTransformerClassifier\u001b[39m\u001b[38;5;124m'\u001b[39m: TransformerClassifier}\n\u001b[1;32m---> 37\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./trytransformer_classifier_model.h5\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Load and preprocess the .svc file\u001b[39;00m\n\u001b[0;32m     40\u001b[0m data \u001b[38;5;241m=\u001b[39m load_single_svc_file(file_path)\n",
      "File \u001b[1;32mc:\\Users\\JM\\Desktop\\Validation Tool\\venv\\Lib\\site-packages\\keras\\src\\saving\\saving_api.py:196\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[0;32m    189\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m saving_lib\u001b[38;5;241m.\u001b[39mload_model(\n\u001b[0;32m    190\u001b[0m         filepath,\n\u001b[0;32m    191\u001b[0m         custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects,\n\u001b[0;32m    192\u001b[0m         \u001b[38;5;28mcompile\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcompile\u001b[39m,\n\u001b[0;32m    193\u001b[0m         safe_mode\u001b[38;5;241m=\u001b[39msafe_mode,\n\u001b[0;32m    194\u001b[0m     )\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(filepath)\u001b[38;5;241m.\u001b[39mendswith((\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.hdf5\u001b[39m\u001b[38;5;124m\"\u001b[39m)):\n\u001b[1;32m--> 196\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlegacy_h5_format\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model_from_hdf5\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mcompile\u001b[39;49m\n\u001b[0;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(filepath)\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.keras\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    200\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    201\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile not found: filepath=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    202\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease ensure the file is an accessible `.keras` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    203\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzip file.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    204\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\JM\\Desktop\\Validation Tool\\venv\\Lib\\site-packages\\keras\\src\\legacy\\saving\\legacy_h5_format.py:133\u001b[0m, in \u001b[0;36mload_model_from_hdf5\u001b[1;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[0;32m    130\u001b[0m model_config \u001b[38;5;241m=\u001b[39m json_utils\u001b[38;5;241m.\u001b[39mdecode(model_config)\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m saving_options\u001b[38;5;241m.\u001b[39mkeras_option_scope(use_legacy_config\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m--> 133\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43msaving_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_from_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    134\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\n\u001b[0;32m    135\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    137\u001b[0m     \u001b[38;5;66;03m# set weights\u001b[39;00m\n\u001b[0;32m    138\u001b[0m     load_weights_from_hdf5_group(f[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_weights\u001b[39m\u001b[38;5;124m\"\u001b[39m], model)\n",
      "File \u001b[1;32mc:\\Users\\JM\\Desktop\\Validation Tool\\venv\\Lib\\site-packages\\keras\\src\\legacy\\saving\\saving_utils.py:85\u001b[0m, in \u001b[0;36mmodel_from_config\u001b[1;34m(config, custom_objects)\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;66;03m# TODO(nkovela): Swap find and replace args during Keras 3.0 release\u001b[39;00m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;66;03m# Replace keras refs with keras\u001b[39;00m\n\u001b[0;32m     83\u001b[0m config \u001b[38;5;241m=\u001b[39m _find_replace_nested_dict(config, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeras.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeras.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 85\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mserialization\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeserialize_keras_object\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     86\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     87\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodule_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMODULE_OBJECTS\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mALL_OBJECTS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     88\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     89\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprintable_module_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlayer\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     90\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\JM\\Desktop\\Validation Tool\\venv\\Lib\\site-packages\\keras\\src\\legacy\\saving\\serialization.py:495\u001b[0m, in \u001b[0;36mdeserialize_keras_object\u001b[1;34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[0;32m    490\u001b[0m cls_config \u001b[38;5;241m=\u001b[39m _find_replace_nested_dict(\n\u001b[0;32m    491\u001b[0m     cls_config, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeras.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeras.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    492\u001b[0m )\n\u001b[0;32m    494\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcustom_objects\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m arg_spec\u001b[38;5;241m.\u001b[39margs:\n\u001b[1;32m--> 495\u001b[0m     deserialized_obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcls_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mobject_registration\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGLOBAL_CUSTOM_OBJECTS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    503\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m object_registration\u001b[38;5;241m.\u001b[39mCustomObjectScope(custom_objects):\n",
      "File \u001b[1;32mc:\\Users\\JM\\Desktop\\Validation Tool\\venv\\Lib\\site-packages\\keras\\src\\models\\model.py:540\u001b[0m, in \u001b[0;36mModel.from_config\u001b[1;34m(cls, config, custom_objects)\u001b[0m\n\u001b[0;32m    538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig)\n\u001b[0;32m    539\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 540\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    541\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to revive model from config. When overriding \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    542\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe `get_config()` method, make sure that the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    543\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturned config contains all items used as arguments \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    544\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min the  constructor to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    545\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhich is the default behavior. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    546\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou can override this default behavior by defining a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    547\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`from_config(cls, config)` class method to specify \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    548\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhow to create an \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    549\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstance of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m from its config.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    550\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived config=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    551\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError encountered during deserialization: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    552\u001b[0m     )\n",
      "\u001b[1;31mTypeError\u001b[0m: Unable to revive model from config. When overriding the `get_config()` method, make sure that the returned config contains all items used as arguments in the  constructor to <class '__main__.TransformerClassifier'>, which is the default behavior. You can override this default behavior by defining a `from_config(cls, config)` class method to specify how to create an instance of TransformerClassifier from its config.\n\nReceived config={'trainable': True, 'dtype': {'module': 'keras', 'class_name': 'DTypePolicy', 'config': {'name': 'float32'}, 'registered_name': None}}\n\nError encountered during deserialization: TransformerClassifier.__init__() got an unexpected keyword argument 'trainable'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Function to load and preprocess a single .svc file for prediction\n",
    "def load_single_svc_file(file_path, expected_features=150):\n",
    "    try:\n",
    "        with open(file_path, 'r') as f:\n",
    "            lines = [line.strip() for line in f.readlines() if line.strip()]\n",
    "\n",
    "        df = pd.DataFrame([line.split() for line in lines])\n",
    "\n",
    "        if df.empty:\n",
    "            raise ValueError(f\"{file_path} is empty after processing.\")\n",
    "\n",
    "        data = df.iloc[1:, :].values\n",
    "        data = np.array(data, dtype=float)\n",
    "\n",
    "        if data.shape[1] > expected_features:\n",
    "            data = data[:, :expected_features]\n",
    "        elif data.shape[1] < expected_features:\n",
    "            padding = np.zeros((data.shape[0], expected_features - data.shape[1]))\n",
    "            data = np.hstack((data, padding))\n",
    "\n",
    "        return data\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Path to the .svc file for prediction\n",
    "file_path = '../test/samplefew/2_hw00004 (2).svc'\n",
    "\n",
    "# Load the custom model\n",
    "custom_objects = {'TransformerClassifier': TransformerClassifier}\n",
    "model = tf.keras.models.load_model('./trytransformer_classifier_model.h5', custom_objects=custom_objects)\n",
    "\n",
    "# Load and preprocess the .svc file\n",
    "data = load_single_svc_file(file_path)\n",
    "\n",
    "if data is not None:\n",
    "    expected_features = 150\n",
    "    data = data.reshape((1, expected_features))\n",
    "\n",
    "    # Predict the class probabilities\n",
    "    predictions = model.predict(data)\n",
    "\n",
    "    # Get the predicted class\n",
    "    predicted_class = np.argmax(predictions, axis=1)\n",
    "\n",
    "    # Output the predicted class\n",
    "    class_labels = ['Depression', 'Anxiety', 'Stress']\n",
    "    print(f\"Predicted emotion: {class_labels[predicted_class[0]]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ba062a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54302da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Function to load and preprocess a single .svc file for prediction\n",
    "def load_single_svc_file(file_path, expected_timesteps=800, num_features=56):\n",
    "    try:\n",
    "        # Load the file and process the lines\n",
    "        with open(file_path, 'r') as f:\n",
    "            lines = [line.strip() for line in f.readlines() if line.strip()]  # Strip whitespace and ignore empty lines\n",
    "\n",
    "        # Process lines into a DataFrame by splitting by whitespace\n",
    "        df = pd.DataFrame([line.split() for line in lines])  # Split by whitespace\n",
    "\n",
    "        if df.empty:\n",
    "            raise ValueError(f\"{file_path} is empty after processing.\")\n",
    "\n",
    "        # Extract the feature data from the rows starting from the second row\n",
    "        data = df.iloc[1:, :].values  # All columns are features\n",
    "\n",
    "        # Convert to a float NumPy array\n",
    "        data = np.array(data, dtype=float)\n",
    "\n",
    "        # Check if we need to pad or truncate data to match (800 timesteps, 56 features)\n",
    "        if data.shape[0] > expected_timesteps:\n",
    "            # If more timesteps, truncate the excess\n",
    "            data = data[:expected_timesteps, :]\n",
    "        elif data.shape[0] < expected_timesteps:\n",
    "            # If fewer timesteps, pad with zeros\n",
    "            padding = np.zeros((expected_timesteps - data.shape[0], num_features))\n",
    "            data = np.vstack((data, padding))\n",
    "\n",
    "        # Ensure that the number of features matches the expected number (56)\n",
    "        if data.shape[1] > num_features:\n",
    "            # Truncate excess features\n",
    "            data = data[:, :num_features]\n",
    "        elif data.shape[1] < num_features:\n",
    "            # Pad missing features with zeros\n",
    "            padding = np.zeros((expected_timesteps, num_features - data.shape[1]))\n",
    "            data = np.hstack((data, padding))\n",
    "\n",
    "        # Return the reshaped data\n",
    "        return data\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Path to the .svc file to predict\n",
    "file_path = '../test/samplefew/2_hw00005(2).svc'\n",
    "\n",
    "# Load the saved model\n",
    "model = tf.keras.models.load_model('./Emotion-detection.model.keras')\n",
    "\n",
    "# Load and preprocess the .svc file\n",
    "data = load_single_svc_file(file_path)\n",
    "\n",
    "# Reshape the data for prediction (1 sample, 800 timesteps, 56 features)\n",
    "if data is not None:\n",
    "    data = data.reshape((1, 800, 56))  # Reshape to match the input shape expected by the model\n",
    "\n",
    "    # Predict the class probabilities\n",
    "    predictions = model.predict(data)\n",
    "\n",
    "    # Get the predicted class (index of the highest probability)\n",
    "    predicted_class = np.argmax(predictions, axis=1)\n",
    "\n",
    "    # Output the predicted class\n",
    "    class_labels = ['Una','Depression', 'Stress']\n",
    "    print(f\"Predicted emotion: {class_labels[predicted_class[0]]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b4484ebb",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unknown layer: 'TransformerClassifier'. Please ensure you are using a `keras.utils.custom_object_scope` and that this object is included in the scope. See https://www.tensorflow.org/guide/keras/save_and_serialize#registering_the_custom_object for details.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 44\u001b[0m\n\u001b[0;32m     41\u001b[0m file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../test/samplefew/2_hw00004 (2).svc\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# Load the saved model\u001b[39;00m\n\u001b[1;32m---> 44\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./testtransformer_classifier_model.h5\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# Load and preprocess the .svc file\u001b[39;00m\n\u001b[0;32m     47\u001b[0m data \u001b[38;5;241m=\u001b[39m load_single_svc_file(file_path)\n",
      "File \u001b[1;32mc:\\Users\\JM\\Desktop\\Validation Tool\\venv\\Lib\\site-packages\\keras\\src\\saving\\saving_api.py:196\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[0;32m    189\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m saving_lib\u001b[38;5;241m.\u001b[39mload_model(\n\u001b[0;32m    190\u001b[0m         filepath,\n\u001b[0;32m    191\u001b[0m         custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects,\n\u001b[0;32m    192\u001b[0m         \u001b[38;5;28mcompile\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcompile\u001b[39m,\n\u001b[0;32m    193\u001b[0m         safe_mode\u001b[38;5;241m=\u001b[39msafe_mode,\n\u001b[0;32m    194\u001b[0m     )\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(filepath)\u001b[38;5;241m.\u001b[39mendswith((\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.hdf5\u001b[39m\u001b[38;5;124m\"\u001b[39m)):\n\u001b[1;32m--> 196\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlegacy_h5_format\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model_from_hdf5\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mcompile\u001b[39;49m\n\u001b[0;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(filepath)\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.keras\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    200\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    201\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile not found: filepath=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    202\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease ensure the file is an accessible `.keras` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    203\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzip file.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    204\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\JM\\Desktop\\Validation Tool\\venv\\Lib\\site-packages\\keras\\src\\legacy\\saving\\legacy_h5_format.py:133\u001b[0m, in \u001b[0;36mload_model_from_hdf5\u001b[1;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[0;32m    130\u001b[0m model_config \u001b[38;5;241m=\u001b[39m json_utils\u001b[38;5;241m.\u001b[39mdecode(model_config)\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m saving_options\u001b[38;5;241m.\u001b[39mkeras_option_scope(use_legacy_config\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m--> 133\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43msaving_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_from_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    134\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\n\u001b[0;32m    135\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    137\u001b[0m     \u001b[38;5;66;03m# set weights\u001b[39;00m\n\u001b[0;32m    138\u001b[0m     load_weights_from_hdf5_group(f[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_weights\u001b[39m\u001b[38;5;124m\"\u001b[39m], model)\n",
      "File \u001b[1;32mc:\\Users\\JM\\Desktop\\Validation Tool\\venv\\Lib\\site-packages\\keras\\src\\legacy\\saving\\saving_utils.py:85\u001b[0m, in \u001b[0;36mmodel_from_config\u001b[1;34m(config, custom_objects)\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;66;03m# TODO(nkovela): Swap find and replace args during Keras 3.0 release\u001b[39;00m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;66;03m# Replace keras refs with keras\u001b[39;00m\n\u001b[0;32m     83\u001b[0m config \u001b[38;5;241m=\u001b[39m _find_replace_nested_dict(config, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeras.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeras.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 85\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mserialization\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeserialize_keras_object\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     86\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     87\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodule_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMODULE_OBJECTS\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mALL_OBJECTS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     88\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     89\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprintable_module_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlayer\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     90\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\JM\\Desktop\\Validation Tool\\venv\\Lib\\site-packages\\keras\\src\\legacy\\saving\\serialization.py:473\u001b[0m, in \u001b[0;36mdeserialize_keras_object\u001b[1;34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[0;32m    470\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(identifier, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    471\u001b[0m     \u001b[38;5;66;03m# In this case we are dealing with a Keras config dictionary.\u001b[39;00m\n\u001b[0;32m    472\u001b[0m     config \u001b[38;5;241m=\u001b[39m identifier\n\u001b[1;32m--> 473\u001b[0m     (\u001b[38;5;28mcls\u001b[39m, cls_config) \u001b[38;5;241m=\u001b[39m \u001b[43mclass_and_config_for_serialized_keras_object\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    474\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_objects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprintable_module_name\u001b[49m\n\u001b[0;32m    475\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    477\u001b[0m     \u001b[38;5;66;03m# If this object has already been loaded (i.e. it's shared between\u001b[39;00m\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;66;03m# multiple objects), return the already-loaded object.\u001b[39;00m\n\u001b[0;32m    479\u001b[0m     shared_object_id \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mget(SHARED_OBJECT_KEY)\n",
      "File \u001b[1;32mc:\\Users\\JM\\Desktop\\Validation Tool\\venv\\Lib\\site-packages\\keras\\src\\legacy\\saving\\serialization.py:354\u001b[0m, in \u001b[0;36mclass_and_config_for_serialized_keras_object\u001b[1;34m(config, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[0;32m    350\u001b[0m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m object_registration\u001b[38;5;241m.\u001b[39mget_registered_object(\n\u001b[0;32m    351\u001b[0m     class_name, custom_objects, module_objects\n\u001b[0;32m    352\u001b[0m )\n\u001b[0;32m    353\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 354\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    355\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprintable_module_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    356\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease ensure you are using a `keras.utils.custom_object_scope` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    357\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand that this object is included in the scope. See \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    358\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://www.tensorflow.org/guide/keras/save_and_serialize\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    359\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#registering_the_custom_object for details.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    360\u001b[0m     )\n\u001b[0;32m    362\u001b[0m cls_config \u001b[38;5;241m=\u001b[39m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    363\u001b[0m \u001b[38;5;66;03m# Check if `cls_config` is a list. If it is a list, return the class and the\u001b[39;00m\n\u001b[0;32m    364\u001b[0m \u001b[38;5;66;03m# associated class configs for recursively deserialization. This case will\u001b[39;00m\n\u001b[0;32m    365\u001b[0m \u001b[38;5;66;03m# happen on the old version of sequential model (e.g. `keras_version` ==\u001b[39;00m\n\u001b[0;32m    366\u001b[0m \u001b[38;5;66;03m# \"2.0.6\"), which is serialized in a different structure, for example\u001b[39;00m\n\u001b[0;32m    367\u001b[0m \u001b[38;5;66;03m# \"{'class_name': 'Sequential',\u001b[39;00m\n\u001b[0;32m    368\u001b[0m \u001b[38;5;66;03m#   'config': [{'class_name': 'Embedding', 'config': ...}, {}, ...]}\".\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: Unknown layer: 'TransformerClassifier'. Please ensure you are using a `keras.utils.custom_object_scope` and that this object is included in the scope. See https://www.tensorflow.org/guide/keras/save_and_serialize#registering_the_custom_object for details."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "\n",
    "# Function to load and preprocess a single .svc file for prediction\n",
    "def load_single_svc_file(file_path, expected_features=150):\n",
    "    try:\n",
    "        # Load the file and process the lines\n",
    "        with open(file_path, 'r') as f:\n",
    "            lines = [line.strip() for line in f.readlines() if line.strip()]  # Strip whitespace and ignore empty lines\n",
    "\n",
    "        # Process lines into a DataFrame by splitting by whitespace\n",
    "        df = pd.DataFrame([line.split() for line in lines])  # Split by whitespace\n",
    "\n",
    "        if df.empty:\n",
    "            raise ValueError(f\"{file_path} is empty after processing.\")\n",
    "\n",
    "        # Extract the feature data from the rows starting from the second row\n",
    "        data = df.iloc[1:, :].values  # All columns are features\n",
    "\n",
    "        # Convert to a float NumPy array\n",
    "        data = np.array(data, dtype=float)\n",
    "\n",
    "        # Ensure the number of features matches the expected number\n",
    "        if data.shape[1] > expected_features:\n",
    "            # Truncate excess features\n",
    "            data = data[:, :expected_features]\n",
    "        elif data.shape[1] < expected_features:\n",
    "            # Pad missing features with zeros\n",
    "            padding = np.zeros((data.shape[0], expected_features - data.shape[1]))\n",
    "            data = np.hstack((data, padding))\n",
    "\n",
    "        # Return the reshaped data (single sample)\n",
    "        return data\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Path to the .svc file to predict\n",
    "file_path = '../test/samplefew/2_hw00004 (2).svc'\n",
    "\n",
    "# Load the saved model\n",
    "model = tf.keras.models.load_model('./testtransformer_classifier_model.h5')\n",
    "\n",
    "# Load and preprocess the .svc file\n",
    "data = load_single_svc_file(file_path)\n",
    "\n",
    "# Reshape the data for prediction (1 sample, expected_features)\n",
    "if data is not None:\n",
    "    # Assuming concatenated features (e.g., 150 features from different domains)\n",
    "    expected_features = 150  # Replace with the actual number from your training data\n",
    "    data = data.reshape((1, expected_features))  # Reshape to match the input shape expected by the model\n",
    "\n",
    "    # Predict the class probabilities\n",
    "    predictions = model.predict(data)\n",
    "\n",
    "    # Get the predicted class (index of the highest probability)\n",
    "    predicted_class = np.argmax(predictions, axis=1)\n",
    "\n",
    "    # Output the predicted class\n",
    "    class_labels = ['Depression', 'Anxiety', 'Stress']\n",
    "    print(f\"Predicted emotion: {class_labels[predicted_class[0]]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e41222",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
