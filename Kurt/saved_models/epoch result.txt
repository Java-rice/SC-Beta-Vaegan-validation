Epoch 1/10
4334/4334 ━━━━━━━━━━━━━━━━━━━━ 745s 167ms/step - accuracy: 0.5791 - loss: 1.8770 - val_accuracy: 0.6353 - val_loss: 0.9029 - learning_rate: 0.0010
Epoch 2/10
4334/4334 ━━━━━━━━━━━━━━━━━━━━ 729s 168ms/step - accuracy: 0.6091 - loss: 0.9779 - val_accuracy: 0.6417 - val_loss: 0.8772 - learning_rate: 0.0010
Epoch 3/10
4334/4334 ━━━━━━━━━━━━━━━━━━━━ 720s 166ms/step - accuracy: 0.6138 - loss: 0.9639 - val_accuracy: 0.6667 - val_loss: 0.8543 - learning_rate: 0.0010
Epoch 4/10
4334/4334 ━━━━━━━━━━━━━━━━━━━━ 673s 155ms/step - accuracy: 0.6155 - loss: 0.9564 - val_accuracy: 0.6354 - val_loss: 0.8954 - learning_rate: 0.0010
Epoch 5/10
4334/4334 ━━━━━━━━━━━━━━━━━━━━ 615s 142ms/step - accuracy: 0.6155 - loss: 0.9549 - val_accuracy: 0.6303 - val_loss: 0.9458 - learning_rate: 0.0010
Epoch 6/10
4334/4334 ━━━━━━━━━━━━━━━━━━━━ 620s 143ms/step - accuracy: 0.6166 - loss: 0.9552 - val_accuracy: 0.6726 - val_loss: 0.8569 - learning_rate: 0.0010
Epoch 7/10
4334/4334 ━━━━━━━━━━━━━━━━━━━━ 613s 141ms/step - accuracy: 0.6260 - loss: 0.9277 - val_accuracy: 0.6597 - val_loss: 0.8357 - learning_rate: 2.0000e-04
Epoch 8/10
4334/4334 ━━━━━━━━━━━━━━━━━━━━ 612s 141ms/step - accuracy: 0.6276 - loss: 0.9202 - val_accuracy: 0.6577 - val_loss: 0.8243 - learning_rate: 2.0000e-04
Epoch 9/10
4334/4334 ━━━━━━━━━━━━━━━━━━━━ 613s 141ms/step - accuracy: 0.6318 - loss: 0.9126 - val_accuracy: 0.6692 - val_loss: 0.8126 - learning_rate: 2.0000e-04
Epoch 10/10
4334/4334 ━━━━━━━━━━━━━━━━━━━━ 614s 142ms/step - accuracy: 0.6305 - loss: 0.9131 - val_accuracy: 0.6563 - val_loss: 0.8279 - learning_rate: 2.0000e-04
5418/5418 ━━━━━━━━━━━━━━━━━━━━ 18s 3ms/step - accuracy: 0.6682 - loss: 0.8209
Test Loss: 0.8235368728637695, Test Accuracy: 0.6673935055732727









num_classes = 4
embed_dim = 128  # Increased embedding dimension
num_heads = 8    # Increased number of attention heads
ff_dim = 256    # Increased feedforward dimension
num_layers = 1   # Increased number of transformer layers
Epoch 1/10
3386/3386 ━━━━━━━━━━━━━━━━━━━━ 186s 53ms/step - accuracy: 0.6485 - loss: 2.0891 - val_accuracy: 0.7952 - val_loss: 0.6559 - learning_rate: 0.0010
Epoch 2/10
 544/3386 ━━━━━━━━━━━━━━━━━━━━ 2:21 50ms/step - accuracy: 0.7211 - loss: 0.7995



num_classes = 4
embed_dim = 128  # Increased embedding dimension
num_heads = 8    # Increased number of attention heads
ff_dim = 256    # Increased feedforward dimension
num_layers = 2   # Increased number of transformer layers
Epoch 1/10
3386/3386 ━━━━━━━━━━━━━━━━━━━━ 499s 145ms/step - accuracy: 0.6510 - loss: 2.8959 - val_accuracy: 0.7445 - val_loss: 0.6920 - learning_rate: 0.0010
Epoch 2/10
 112/3386 ━━━━━━━━━━━━━━━━━━━━ 7:30 138ms/step - accuracy: 0.7179 - loss: 0.8016



6min per Epoch
num_classes = 4
embed_dim = 256  # Increased embedding dimension
num_heads = 8    # Increased number of attention heads
ff_dim = 256    # Increased feedforward dimension
num_layers = 1 

 Epoch 1/10
3386/3386 ━━━━━━━━━━━━━━━━━━━━ 346s 101ms/step - accuracy: 0.6585 - loss: 2.4567 - val_accuracy: 0.7893 - val_loss: 0.6389 - learning_rate: 0.0010
Epoch 2/10
3386/3386 ━━━━━━━━━━━━━━━━━━━━ 331s 98ms/step - accuracy: 0.7232 - loss: 0.7939 - val_accuracy: 0.7701 - val_loss: 0.6556 - learning_rate: 0.0010
Epoch 3/10
 475/3386 ━━━━━━━━━━━━━━━━━━━━ 4:46 98ms/step - accuracy: 0.7325 - loss: 0.7660



2min per eppoch
num_classes = 4
embed_dim = 128  # Increased embedding dimension
num_heads = 4    # Increased number of attention heads
ff_dim = 256    # Increased feedforward dimension
num_layers = 1  
batchsize 32
 Epoch 1/10
6771/6771 ━━━━━━━━━━━━━━━━━━━━ 106s 14ms/step - accuracy: 0.5950 - loss: 1.7927 - val_accuracy: 0.6276 - val_loss: 0.9783 - learning_rate: 0.0010
Epoch 2/10
6771/6771 ━━━━━━━━━━━━━━━━━━━━ 99s 15ms/step - accuracy: 0.6294 - loss: 0.9047 - val_accuracy: 0.7257 - val_loss: 0.7510 - learning_rate: 0.0010
Epoch 3/10
2520/6771 ━━━━━━━━━━━━━━━━━━━━ 55s 13ms/step - accuracy: 0.6421 - loss: 0.8880



1:30 mins per epoch
num_classes = 4
embed_dim = 128  # Increased embedding dimension
num_heads = 8    # Increased number of attention heads
ff_dim = 256    # Increased feedforward dimension
num_layers = 1

Epoch 1/10
6771/6771 ━━━━━━━━━━━━━━━━━━━━ 88s 12ms/step - accuracy: 0.5973 - loss: 1.7764 - val_accuracy: 0.7046 - val_loss: 0.7902 - learning_rate: 0.0010
Epoch 2/10
6771/6771 ━━━━━━━━━━━━━━━━━━━━ 111s 16ms/step - accuracy: 0.6370 - loss: 0.8929 - val_accuracy: 0.6634 - val_loss: 0.7715 - learning_rate: 0.0010
Epoch 3/10
6771/6771 ━━━━━━━━━━━━━━━━━━━━ 97s 14ms/step - accuracy: 0.6484 - loss: 0.8764 - val_accuracy: 0.7357 - val_loss: 0.7561 - learning_rate: 0.0010
Epoch 4/10
2254/6771 ━━━━━━━━━━━━━━━━━━━━ 1:00 13ms/step - accuracy: 0.6520 - loss: 0.8769



2min per epoch
num_classes = 4
embed_dim = 128  # Increased embedding dimension
num_heads = 8    # Increased number of attention heads
ff_dim = 256    # Increased feedforward dimension
num_layers = 1
bs=8
Epoch 1/10
27084/27084 ━━━━━━━━━━━━━━━━━━━━ 136s 5ms/step - accuracy: 0.5025 - loss: 1.5384 - val_accuracy: 0.5560 - val_loss: 1.0000 - learning_rate: 0.0010
Epoch 2/10
 4729/27084 ━━━━━━━━━━━━━━━━━━━━ 1:41 5ms/step - accuracy: 0.5107 - loss: 1.1389






 1:40 per epoch
num_classes = 4
embed_dim = 128  # Increased embedding dimension
num_heads = 8    # Increased number of attention heads
ff_dim = 256    # Increased feedforward dimension
num_layers = 1
bs=32
Epoch 1/10
6771/6771 ━━━━━━━━━━━━━━━━━━━━ 101s 14ms/step - accuracy: 0.6001 - loss: 1.7940 - val_accuracy: 0.7098 - val_loss: 0.7901 - learning_rate: 0.0010
Epoch 2/10
2262/6771 ━━━━━━━━━━━━━━━━━━━━ 1:11 16ms/step - accuracy: 0.6330 - loss: 0.9073



4mins epoch
bs=64
Epoch 1/10
3386/3386 ━━━━━━━━━━━━━━━━━━━━ 187s 54ms/step - accuracy: 0.6520 - loss: 2.0846 - val_accuracy: 0.8010 - val_loss: 0.6267 - learning_rate: 0.0010
Epoch 2/10
2618/3386 ━━━━━━━━━━━━━━━━━━━━ 38s 50ms/step - accuracy: 0.7263 - loss: 0.7883