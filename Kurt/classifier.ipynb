{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1 - Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.saving import register_keras_serializable\n",
    "import joblib  # For loading the saved scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2 - Define Transformer model classes (needed for loading the saved model)\n",
    "@register_keras_serializable()\n",
    "class MultiHeadSelfAttention(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, dropout_rate=0.1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout_rate = dropout_rate\n",
    "        if embed_dim % num_heads != 0:\n",
    "            raise ValueError(f\"embedding dimension = {embed_dim} should be divisible by number of heads = {num_heads}\")\n",
    "        self.projection_dim = embed_dim // num_heads\n",
    "        self.query_dense = layers.Dense(embed_dim, kernel_regularizer=l2(0.01))\n",
    "        self.key_dense = layers.Dense(embed_dim, kernel_regularizer=l2(0.01))\n",
    "        self.value_dense = layers.Dense(embed_dim, kernel_regularizer=l2(0.01))\n",
    "        self.combine_heads = layers.Dense(embed_dim, kernel_regularizer=l2(0.01))\n",
    "        self.dropout = layers.Dropout(dropout_rate)\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"embed_dim\": self.embed_dim,\n",
    "            \"num_heads\": self.num_heads,\n",
    "            \"dropout_rate\": self.dropout_rate,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    def attention(self, query, key, value):\n",
    "        score = tf.matmul(query, key, transpose_b=True)\n",
    "        dim_key = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "        scaled_score = score / tf.math.sqrt(dim_key)\n",
    "        weights = tf.nn.softmax(scaled_score, axis=-1)\n",
    "        weights = self.dropout(weights)\n",
    "        output = tf.matmul(weights, value)\n",
    "        return output, weights\n",
    "\n",
    "    def separate_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        query = self.query_dense(inputs)\n",
    "        key = self.key_dense(inputs)\n",
    "        value = self.value_dense(inputs)\n",
    "        query = self.separate_heads(query, batch_size)\n",
    "        key = self.separate_heads(key, batch_size)\n",
    "        value = self.separate_heads(value, batch_size)\n",
    "        attention, weights = self.attention(query, key, value)\n",
    "        attention = tf.transpose(attention, perm=[0, 2, 1, 3])\n",
    "        concat_attention = tf.reshape(attention, (batch_size, -1, self.embed_dim))\n",
    "        output = self.combine_heads(concat_attention)\n",
    "        return output\n",
    "\n",
    "@register_keras_serializable()\n",
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.2, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.ff_dim = ff_dim\n",
    "        self.rate = rate\n",
    "        self.att = MultiHeadSelfAttention(embed_dim, num_heads, rate)\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            layers.Dense(ff_dim, activation=\"relu\", kernel_regularizer=l2(0.01)),\n",
    "            layers.Dropout(rate),\n",
    "            layers.Dense(embed_dim, kernel_regularizer=l2(0.01))\n",
    "        ])\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"embed_dim\": self.embed_dim,\n",
    "            \"num_heads\": self.num_heads,\n",
    "            \"ff_dim\": self.ff_dim,\n",
    "            \"rate\": self.rate,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        attn_output = self.att(inputs, training=training)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "@register_keras_serializable()\n",
    "class TransformerClassifier(Model):\n",
    "    def __init__(self, num_classes, embed_dim, num_heads, ff_dim, num_layers, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_classes = num_classes\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.ff_dim = ff_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.input_norm = layers.BatchNormalization()\n",
    "        self.dense_input = layers.Dense(embed_dim, kernel_regularizer=l2(0.01))\n",
    "        self.input_dropout = layers.Dropout(0.2)\n",
    "        \n",
    "        self.transformer_blocks = [\n",
    "            TransformerBlock(embed_dim, num_heads, ff_dim) for _ in range(num_layers)\n",
    "        ]\n",
    "        \n",
    "        self.global_average_pooling = layers.GlobalAveragePooling1D()\n",
    "        self.dropout1 = layers.Dropout(0.2)\n",
    "        self.dense1 = layers.Dense(128, activation='relu', kernel_regularizer=l2(0.01))\n",
    "        self.dropout2 = layers.Dropout(0.2)\n",
    "        self.dense2 = layers.Dense(num_classes, activation='softmax', kernel_regularizer=l2(0.01))\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"num_classes\": self.num_classes,\n",
    "            \"embed_dim\": self.embed_dim,\n",
    "            \"num_heads\": self.num_heads,\n",
    "            \"ff_dim\": self.ff_dim,\n",
    "            \"num_layers\": self.num_layers,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        x = self.input_norm(inputs)\n",
    "        x = self.dense_input(x)\n",
    "        x = self.input_dropout(x, training=training)\n",
    "        \n",
    "        for transformer in self.transformer_blocks:\n",
    "            x = transformer(x, training=training)\n",
    "            \n",
    "        x = self.global_average_pooling(x)\n",
    "        x = self.dropout1(x, training=training)\n",
    "        x = self.dense1(x)\n",
    "        x = self.dropout2(x, training=training)\n",
    "        return self.dense2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3 - Define feature extraction functions\n",
    "def extract_time_domain_features(data):\n",
    "    \"\"\"Extract time domain features from the input data\"\"\"\n",
    "    means = np.mean(data, axis=1)\n",
    "    stds = np.std(data, axis=1)\n",
    "    mins = np.min(data, axis=1)\n",
    "    maxs = np.max(data, axis=1)\n",
    "    percentile_25 = np.percentile(data, 25, axis=1)\n",
    "    percentile_50 = np.percentile(data, 50, axis=1)\n",
    "    percentile_75 = np.percentile(data, 75, axis=1)\n",
    "    \n",
    "    time_features = np.column_stack([\n",
    "        means, stds, mins, maxs,\n",
    "        percentile_25, percentile_50, percentile_75\n",
    "    ])\n",
    "    \n",
    "    return time_features\n",
    "\n",
    "def extract_frequency_domain_features(data):\n",
    "    \"\"\"Extract frequency domain features from the input data\"\"\"\n",
    "    freq_features = np.fft.fft(data, axis=1)\n",
    "    freq_magnitude = np.abs(freq_features)\n",
    "    \n",
    "    dominant_freqs = np.argmax(freq_magnitude, axis=1)\n",
    "    freq_energies = np.sum(freq_magnitude, axis=1)\n",
    "    \n",
    "    freq_features = np.column_stack([dominant_freqs, freq_energies])\n",
    "    \n",
    "    return freq_features\n",
    "\n",
    "def extract_statistical_features(data):\n",
    "    \"\"\"Extract statistical features from the input data\"\"\"\n",
    "    means = np.mean(data, axis=1)\n",
    "    medians = np.median(data, axis=1)\n",
    "    variances = np.var(data, axis=1)\n",
    "    skewness = skew(data, axis=1)\n",
    "    kurtosis_vals = kurtosis(data, axis=1)\n",
    "    \n",
    "    statistical_features = np.column_stack([\n",
    "        means, medians, variances, skewness, kurtosis_vals\n",
    "    ])\n",
    "    \n",
    "    return statistical_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4 - Define preprocessing and classification functions\n",
    "def load_and_preprocess_file(file_path, scaler):\n",
    "    \"\"\"\n",
    "    Load and preprocess a single file for classification\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Read the file\n",
    "        with open(file_path, 'r') as f:\n",
    "            lines = [line.strip() for line in f.readlines() if line.strip()]\n",
    "\n",
    "        # Convert to DataFrame\n",
    "        df = pd.DataFrame([line.split() for line in lines])\n",
    "        \n",
    "        if df.empty:\n",
    "            raise ValueError(f\"File is empty after processing\")\n",
    "\n",
    "        # First row contains total number of rows\n",
    "        total_rows = int(df.iloc[0, 0])\n",
    "        \n",
    "        # Extract feature data\n",
    "        data = df.iloc[1:, :].values\n",
    "        \n",
    "        # Ensure each row has 7 features\n",
    "        reshaped_data = []\n",
    "        for row in data:\n",
    "            if len(row) == 7:\n",
    "                reshaped_data.append(row)\n",
    "        \n",
    "        if not reshaped_data:\n",
    "            raise ValueError(\"No valid data rows found\")\n",
    "        \n",
    "        # Convert to numpy array\n",
    "        reshaped_data = np.array(reshaped_data, dtype=float)\n",
    "        \n",
    "        # Extract features\n",
    "        time_features = extract_time_domain_features(reshaped_data)\n",
    "        freq_features = extract_frequency_domain_features(reshaped_data)\n",
    "        stat_features = extract_statistical_features(reshaped_data)\n",
    "        \n",
    "        # Combine features\n",
    "        combined_features = np.concatenate((time_features, freq_features, stat_features), axis=1)\n",
    "        combined_features = scaler.transform(combined_features)\n",
    "        return combined_features\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Error processing file {file_path}: {e}\")\n",
    "\n",
    "def classify_emotion(model_path, scaler_path, file_path):\n",
    "    \"\"\"\n",
    "    Classify emotion using saved model and scaler and return confidences for all emotions\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load the saved model and scaler\n",
    "        model = load_model(model_path)\n",
    "        scaler = joblib.load(scaler_path)\n",
    "        \n",
    "        # Process the input file\n",
    "        features = load_and_preprocess_file(file_path, scaler)\n",
    "        \n",
    "        # Make prediction\n",
    "        prediction = model.predict(features)\n",
    "        \n",
    "        # Calculate mean probabilities across all sequences\n",
    "        mean_probabilities = np.mean(prediction, axis=0)\n",
    "        \n",
    "        # Map class indices to labels\n",
    "        class_labels = ['Normal', 'Depression', 'Anxiety', 'Stress']\n",
    "        \n",
    "        # Create sorted list of (emotion, confidence) tuples\n",
    "        emotion_confidences = [\n",
    "            (label, float(prob * 100))\n",
    "            for label, prob in zip(class_labels, mean_probabilities)\n",
    "        ]\n",
    "        # Sort by confidence in descending order\n",
    "        emotion_confidences.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # The predicted emotion is the first one (highest confidence)\n",
    "        predicted_emotion = emotion_confidences[0][0]\n",
    "        \n",
    "        return predicted_emotion, emotion_confidences\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Classification error: {e}\")\n",
    "\n",
    "def classify_directory(model_path, scaler_path, directory_path):\n",
    "    \"\"\"\n",
    "    Classify all .svc files in a directory\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # Find all .svc files\n",
    "    file_paths = glob.glob(os.path.join(directory_path, \"*.svc\"))\n",
    "    \n",
    "    if not file_paths:\n",
    "        raise ValueError(\"No .svc files found in the directory\")\n",
    "    \n",
    "    for file_path in file_paths:\n",
    "        try:\n",
    "            predicted_emotion, emotion_confidences = classify_emotion(\n",
    "                model_path, scaler_path, file_path\n",
    "            )\n",
    "            results.append({\n",
    "                'file': os.path.basename(file_path),\n",
    "                'predicted_emotion': predicted_emotion,\n",
    "                'confidences': emotion_confidences\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_path}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 21ms/step\n",
      "\n",
      "File: 0_hw00001.svc\n",
      "Predicted Emotion: Normal\n",
      "\n",
      "Confidence Levels (Highest to Lowest):\n",
      "Normal: 99.74%\n",
      "Stress: 0.20%\n",
      "Depression: 0.05%\n",
      "Anxiety: 0.00%\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step\n",
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step\n",
      "\n",
      "Directory Classification Results:\n",
      "================================\n",
      "\n",
      "File: 0_hw00001.svc\n",
      "Predicted Emotion: Normal\n",
      "\n",
      "Confidence Levels (Highest to Lowest):\n",
      "Normal: 99.74%\n",
      "Stress: 0.20%\n",
      "Depression: 0.05%\n",
      "Anxiety: 0.00%\n",
      "\n",
      "File: 1_hw00001.svc\n",
      "Predicted Emotion: Depression\n",
      "\n",
      "Confidence Levels (Highest to Lowest):\n",
      "Depression: 99.46%\n",
      "Normal: 0.49%\n",
      "Anxiety: 0.04%\n",
      "Stress: 0.00%\n",
      "\n",
      "File: 2_hw00003.svc\n",
      "Predicted Emotion: Anxiety\n",
      "\n",
      "Confidence Levels (Highest to Lowest):\n",
      "Anxiety: 99.82%\n",
      "Stress: 0.15%\n",
      "Depression: 0.02%\n",
      "Normal: 0.01%\n",
      "\n",
      "File: 3_synthetichw00006.svc\n",
      "Predicted Emotion: Anxiety\n",
      "\n",
      "Confidence Levels (Highest to Lowest):\n",
      "Anxiety: 99.90%\n",
      "Stress: 0.05%\n",
      "Depression: 0.04%\n",
      "Normal: 0.02%\n",
      "\n",
      "File: hw00002.svc\n",
      "Predicted Emotion: Stress\n",
      "\n",
      "Confidence Levels (Highest to Lowest):\n",
      "Stress: 99.81%\n",
      "Normal: 0.15%\n",
      "Anxiety: 0.04%\n",
      "Depression: 0.00%\n",
      "\n",
      "File: u00067s00001_hw00005.svc\n",
      "Predicted Emotion: Depression\n",
      "\n",
      "Confidence Levels (Highest to Lowest):\n",
      "Depression: 99.60%\n",
      "Normal: 0.35%\n",
      "Anxiety: 0.04%\n",
      "Stress: 0.00%\n",
      "\n",
      "File: u00075s00001_hw00002.svc\n",
      "Predicted Emotion: Stress\n",
      "\n",
      "Confidence Levels (Highest to Lowest):\n",
      "Stress: 97.46%\n",
      "Anxiety: 2.46%\n",
      "Normal: 0.07%\n",
      "Depression: 0.01%\n"
     ]
    }
   ],
   "source": [
    "# Cell 5 - Run the classifier\n",
    "# Set the paths\n",
    "model_path = './saved_models/emotion_classifier.keras'\n",
    "scaler_path = './saved_models/scaler.save'\n",
    "input_directory = './input_files/'\n",
    "\n",
    "try:\n",
    "    # Example 1: Classify a single file\n",
    "    file_path = './input_files/0_hw00001.svc'\n",
    "    predicted_emotion, confidences = classify_emotion(model_path, scaler_path, file_path)\n",
    "    print(f\"\\nFile: {os.path.basename(file_path)}\")\n",
    "    print(f\"Predicted Emotion: {predicted_emotion}\")\n",
    "    print(\"\\nConfidence Levels (Highest to Lowest):\")\n",
    "    for emotion, confidence in confidences:\n",
    "        print(f\"{emotion}: {confidence:.2f}%\")\n",
    "    \n",
    "    # Example 2: Classify all files in a directory\n",
    "    results = classify_directory(model_path, scaler_path, input_directory)\n",
    "    \n",
    "    print(\"\\nDirectory Classification Results:\")\n",
    "    print(\"================================\")\n",
    "    for result in results:\n",
    "        print(f\"\\nFile: {result['file']}\")\n",
    "        print(f\"Predicted Emotion: {result['predicted_emotion']}\")\n",
    "        print(\"\\nConfidence Levels (Highest to Lowest):\")\n",
    "        for emotion, confidence in result['confidences']:\n",
    "            print(f\"{emotion}: {confidence:.2f}%\")\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"Classification error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
